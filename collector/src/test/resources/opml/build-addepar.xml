<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Build@Addepar - Medium]]></title>
        <description><![CDATA[Addepar Engineering + Design Blog - Medium]]></description>
        <link>https://medium.com/build-addepar?source=rss----596e43e5e150---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Build@Addepar - Medium</title>
            <link>https://medium.com/build-addepar?source=rss----596e43e5e150---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sat, 02 Jun 2018 09:44:08 GMT</lastBuildDate>
        <atom:link href="https://medium.com/feed/build-addepar" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Ember CLI Addon Docs: Shared Documentation for the Ember Ecosystem]]></title>
            <link>https://medium.com/build-addepar/ember-cli-addon-docs-shared-documentation-for-the-ember-ecosystem-6f29aa0cee87?source=rss----596e43e5e150---4</link>
            <guid isPermaLink="false">https://medium.com/p/6f29aa0cee87</guid>
            <category><![CDATA[ember]]></category>
            <category><![CDATA[emberjs]]></category>
            <category><![CDATA[ember-cli]]></category>
            <category><![CDATA[documentation]]></category>
            <dc:creator><![CDATA[pzuraq]]></dc:creator>
            <pubDate>Wed, 07 Mar 2018 21:00:36 GMT</pubDate>
            <atom:updated>2018-03-07T21:00:36.975Z</atom:updated>
            <content:encoded><![CDATA[<p>On our road to modern Ember here at Addepar, we’ve been hard at work developing a new component library to replace a lot of our older components, such as the now deprecated <a href="https://github.com/Addepar/ember-widgets/">Ember Widgets</a> library. As anyone who has built a shared toolset of components will know, one of the more tedious tasks faced by a library developer is documentation — users of the new components need to be able to reference the APIs to know how to use them, and having a single source of truth for your docs is invaluable to developer productivity, but setting up docs in the first place is a lot of hard work!</p><p>For Ember, part of the problem is that there is no standard documentation solution as of yet. The core Ember and Ember-CLI API docs use YUIDoc to generate docs automatically, but it is an aging library which doesn’t have much support for modern javascript features. JSDoc is also falling behind the times lately, and newer doc tools like ESDoc and Documentation.js are still too experimental for widespread use. Moreover, none of these tools targets Ember — they are meant for generic Javascript usage and don’t highlight things that Ember devs need to know, like what arguments a component receives, what actions it sends, or what values it yields.</p><p>Worse yet, none of these tools has support for live examples and demos. To get those, most addon authors resort to building an Ember app, usually using the test application (or Dummy app) included with the addon. Building an app takes time and effort, and in the end this means that very few addons in the ecosystem are fully documented with good examples and well described APIs.</p><p>When we started to set up our documentation we ran into all of these issues, and we began wondering where the shared solution was for Ember documentation. As often happens in the Ember community, it turned out others had been wondering this as well! We joined forces with Sam Selikoff, Dan Freeman, and the Ember Learning team to work on <a href="https://ember-learn.github.io/ember-cli-addon-docs/latest/"><strong>ember-cli-addon-docs</strong></a>, a project whose goal is to provide a simple and unified way to document all Ember addons. The results have been amazing so far, and with v1.0.0 approaching quickly it supports:</p><ul><li>Markdown-based templates</li><li>Snippets and live demos</li><li>Unified styles and components</li><li>Versioned documentation</li><li>Automated deployments</li><li>Full text search of the guides and the API docs</li><li>Generated API docs with a plugin system for multiple backing generators (Currently supports YUIDoc and ESDoc)</li><li>API docs for Ember concepts, like components</li></ul><p>All of this gets wrapped up in your dummy application with minimal setup, so in the end, it’s still Just an Ember App — you can customize the look and feel and functionality however you choose!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*8nD1eUPx6zRbiLecy2NzaQ.png" /></figure><h3>Installation</h3><p>Just like any other Ember addon, you can add ember-cli-addon-docs to your project using the ember install command:</p><pre>ember install ember-cli-addon-docs</pre><p>This will add a deploy config for deploying your docs to Github pages, and also add the default YUIDoc docs generator. Follow the <a href="https://ember-learn.github.io/ember-cli-addon-docs/latest/docs/quickstart">rest of the quickstart</a> guide to get your docs up and running in no time.</p><h3>Writing Your Docs</h3><p>Documentation pages are written using Markdown, which gets compiled to Handlebars templates to make writing documentation super easy. Just add markdown files instead of templates for your routes and they’ll be compiled without any additional configuration:</p><pre><strong># Installation</strong><br><br>To install My Addon, run...</pre><p>Using components in the markdown files should Just Work, as should helpers and other arbitrary HTML:</p><pre><strong>## My Component demo</strong><br><br>Here&#39;s a demo of it working:<br><br>{{my-component-demo-1}}</pre><p>Check out the <a href="https://ember-learn.github.io/ember-cli-addon-docs/latest/docs/patterns">patterns section of the documentation</a> to see the recommended best practices for organizing your docs.</p><h4>Demos, Snippets, and More</h4><p>Ember-cli-addon-docs also comes with a bunch of components to help you write examples, including a nifty demo component:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*LKasUx1F3p1nVNg4urh0lA.png" /></figure><p>The component uses contextual components to allow you to define your example and render it at the same time, so no need to duplicate code from one place to the other. The addon also comes with built in snippet support using <a href="https://github.com/ef4/ember-code-snippet">ember-code-snippet</a>, so you can add arbitrary snippets if you want:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*9gDh6lQDsDzkB2dYxS9DYw.png" /></figure><p>It also has a live demo component that allows you to create demos users can change on the fly, allowing them to experiment and test out your code in the documentation:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*J_cuCo4aH7ox-iCvzO-ufg.png" /></figure><p>These components give you an amazing toolset for quickly and easily scaffolding up rich examples for your library, and make communicating your features easier than ever!</p><h3>API Documentation</h3><p>The ability to use markdown and a toolset of components makes creating a full-fledged addon site easier than ever. However, they still require a fair amount of effort to maintain, and it’s hard to add detailed descriptions of each component, class, function, and variable by hand. Automated API documentation comes in handy here, and ember-cli-addon-docs provides a modern approach to making the best generated docs for Ember users out there.</p><h4>Made For ES2018+</h4><p>The API docs are modeled to reflect modern Javascript paradigms. Modules are first class citizens, and the existing generators automatically figure out module structure so you don’t need to manually tag them (even with YUIDoc)!</p><p>Modules can document Classes, Functions, and Variables, just like native javascript. Classes can document their Fields, Methods, and Accessors (getters/setters), and all of these things have support for arbitrary tags and decorators, which can be parsed by ESDoc and other modern generators.</p><p>If you want to mix and match modern ES class syntax with the older Ember object model, you can run multiple doc generators side-by-side, meaning you can incrementally convert your project from the old standards to the new standards without a gap in documentation.</p><h4>Made For Ember</h4><p>The API docs are crafted specifically to match the structure of Ember applications. The navigation panel reflects this, displaying a list of the “global” resolved types such as Components, Services, Controllers, Helpers, Routes, and more. Components also have Arguments and Yields:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*3-2wfBGSesQM2qvCD6atQQ.png" /></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*6xGfSAdEaQhp5fs69tax2g.png" /></figure><p>Using tags or decorators, you can document what your users should pass into a component and what they can expect when using its block form. This is much more valuable information to most users than the Methods, Fields, and Accessors of a component, which are, generally speaking, “private” implementation details: unless they extend your component, most users are never going to know about them.</p><h3>Automated Everything</h3><p>Ember-cli-addon-docs supports <a href="https://ember-learn.github.io/ember-cli-addon-docs/latest/docs/deploying">versioned documentation and automated deployments</a> to Github Pages out of the box, with minimal setup required. Older versions of your docs pages are built and stored in a subdirectory in pages, along with a latest and master version by default which allow you to always point people to the current stable and bleeding edge releases. A search index is also automatically generated for your entire site, allowing users to quickly find exactly what they’re looking for.</p><p>Features like these prevent you from having to do the hard work to maintain versioning in your docs and keep then easily navigable. The addon is dedicated to automating as much of the process of generating documentation as possible, so you don’t have to!</p><h3>Towards the Future</h3><p>Ember-cli-addon-docs is still pre-1.0.0, but it’s evolving quickly and should reach a stable release soon. In the future, we’d like to continue evolving its features and making it even easier to bring documentation to the Ember ecosystem.</p><p>When you look at other communities out there to emulate in this realm, the greatest example is probably the <a href="https://docs.rs/">Rust Documentation</a>. Every time a Rust package is built and published, its versioned API docs are built and published as well:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*owXfYiVQ2Dy71ewQDrCDaQ.png" /></figure><p>That level of simplicity and automation is simply amazing. The entire ecosystem can quickly and easily find the docs for just about any package available, and maintainers have little-to-no setup or maintenance required on their end.</p><p>This type of community support is what <strong>ember-cli-addon-docs</strong> is aiming to achieve in the long run. By standardizing on a shared documentation solution, we can also standardize the way we share and access that documentation and make it easier for every Ember developer to stop worrying about writing docs and just get things done!</p><p>Big thanks to <a href="https://medium.com/u/4bebdc004c4c">Sam Selikoff</a> and <a href="https://twitter.com/SchiefGelaufen">Dan Freeman</a>, who started this project and did all of the work on the design, components, markdown compilation, and deployment logic, and to the Ember Learning team for support and inspiration!</p><p>If you’re looking for advice in using the library, would like to provide feedback and track progress, or would like to help contribute, feel free to open up an <a href="https://github.com/ember-learn/ember-cli-addon-docs">issue on Github</a> or to join us in #ec-addon-docs in the <a href="https://ember-community-slackin.herokuapp.com/">Ember Community Slack</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6f29aa0cee87" width="1" height="1"><hr><p><a href="https://medium.com/build-addepar/ember-cli-addon-docs-shared-documentation-for-the-ember-ecosystem-6f29aa0cee87">Ember CLI Addon Docs: Shared Documentation for the Ember Ecosystem</a> was originally published in <a href="https://medium.com/build-addepar">Build@Addepar</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Art of Automation: Striking a Balance]]></title>
            <link>https://medium.com/build-addepar/art-of-automation-striking-a-balance-863d22a0b437?source=rss----596e43e5e150---4</link>
            <guid isPermaLink="false">https://medium.com/p/863d22a0b437</guid>
            <category><![CDATA[automation]]></category>
            <category><![CDATA[fintech]]></category>
            <category><![CDATA[data]]></category>
            <dc:creator><![CDATA[Matt Zukowski]]></dc:creator>
            <pubDate>Thu, 30 Nov 2017 14:06:06 GMT</pubDate>
            <atom:updated>2017-11-30T18:14:11.387Z</atom:updated>
            <content:encoded><![CDATA[<p>As engineers, we want to automate everything. It’s one of our core values at <a href="https://addepar.com/">Addepar</a>. There’s nothing more satisfying than seeing a huge collection of data neatly line up exactly the way you want it to, with every edge case taken care of and every peculiarity accounted for. The tricky thing, though, is that we’re solving the problems of the financial world — a system that is inherently made up of and built by <em>people</em>. It’s laden with incentives for creativity, and we see those sparks of innovation in new products and transactions. This ingenuity is what makes finance special. But the state of perpetual flux also makes full automation impossible.</p><p>At Addepar, <a href="https://medium.com/build-addepar/building-at-addepar-31660bd9ab42">we’ve decided to embrace the mindset of the financial world</a>. We want to allow people to flex their cognitive and deductive skills, while still using automation as a guiding mechanism. We want to stay flexible and yet harness the power of computers, just like the people-centered system we’re trying to model. This approach lets us stay up-to-date. In this blog post, I’m going to dive into one of the most complex and variable financial transactions we encounter at Addepar — corporate actions — as well as some of the lessons we’ve learned about incorporating human insight into our technology.</p><h3>The Challenge of Automating Variations</h3><p>Corporate reorganizations occur often. They can be some of the most high-profile and financially impactful things you hear about: <a href="https://techcrunch.com/2016/12/08/microsoft-officially-closes-its-26-2b-acquisition-of-linkedin/">Microsoft purchasing LinkedIn</a>, <a href="https://www.nytimes.com/2017/06/16/business/dealbook/amazon-whole-foods.html">Amazon purchasing Whole Foods</a>. While certain variations are quite common, these transactions can, and do, get arbitrarily complex. There’s a great deal of variance from one to another and it’s no mystery why — the flexibility on what can be done is very high. Of course, you have certain legal constraints (based on the existing assets/liabilities of the company), operational constraints (the more complicated the transaction, the more costly), and then the hurdle of getting board approval. Beyond that, though, it’s pretty much an open field. We know we can’t predict every possibility; the only guarantee is that we’re going to encounter something we haven’t seen before. Let’s go back in time a little bit and examine a corporate action that was, at the time, uncharted territory.</p><h3>The Corporate Action That Put Our Methods to the Test</h3><p>On April 15th, 2016, Liberty Media Corporation went through a reorganization that our system wasn’t really equipped to handle. Existing common stock was split up into three new distinct securities, at various ratios. The <a href="http://ir.libertymedia.com/releasedetail.cfm?ReleaseID=965402">press release</a> goes into the fine details of the transaction, but the basic flow is outlined below. This is an <em>excellent</em> example of a company doing whatever it wants to and, correspondingly, why it is so hard to automatically process these transactions.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ABebaocBvpQmQPq2RMbfDw.png" /></figure><p>The thing to take note of here is that this transaction was represented in an ever so slightly different way by each of our hundred plus integration points. Naturally, being engineers, we went right to work on automating all of it. The result was an incredibly long and arduous day. A quick search reveals that we have dozens of specific methods or variable names relating to this corporate action. We wrote a lot of code and, frankly, I’m skeptical that any of those code paths have been hit since that day. It was an impactful event for us, though, because it caused us to rethink the way we approach these problems altogether. Analysts were able to look at these files and know in seconds how different pieces tied together. Engineers, however, found it difficult to automate all of their deductions just by using the information in the files.</p><p>We sat down and thought about the problem. We wanted to learn from this experience and apply those findings to the way we approached corporate actions in the future. What were the parts we could always automate? At what point would it make sense to involve a human to provide guidance? The goal was to find a sweet spot where we could stay fully automated except in those cases where potential ambiguity lay.</p><p>Let’s take a quick step back here and look at the components of a typical transaction.</p><h3>Understanding the Unknowns Caused by Potential Variations</h3><p>In every one of the hundreds of thousands of transactions Addepar processes every day, there are two main things we aim to get right: the accounting portion and the performance, or grouping, portion. In order to do that, we need to formulate all possible interpretations of our incoming data. This means taking stock of both the information we’re getting and the information we’re not. We’ll talk later about which parts can be easily automated and which parts can’t. First, let’s think through an example to demonstrate what these two components mean in the financial world.</p><p>You are walking down the street with two whole dollars in your pocket. A trinket from a street merchant catches your eye and you purchase it for one dollar. You now have one dollar and one trinket. The accounting portion here of this transaction is twofold: the gain of a trinket and the loss of a dollar. The grouping portion is the part that ties these two actions together for you. The dollar didn’t just disappear and the trinket didn’t just materialize — these two things are inextricably linked to each other.</p><p>This is one of many problems we’re faced with every single day in the world of performance reporting. These two things are the inputs to many of our calculations and thus getting them right is critical. If we don’t, well… let’s see what happens if we don’t group them.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*zGcN4M5PSxnEoyiROW0Stg.png" /></figure><p>These are the performance charts for cash and for the trinket. We can see that the performance for cash isn’t great. It looks like, in a single instant, we lost half of the value of our cash for no apparent reason. It was worth $2, but is now worth $1. At the same time, a trinket appeared out of nowhere, causing an infinite return. Something that used to be worth $0 is now worth $1!</p><p>What should these charts actually look like?</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*gxgwTswrGRkCE7nAjoU-YA.png" /></figure><p>The value of cash or of a trinket doesn’t change just because it is exchanged for something and the performance chart should reflect that. Grouping this exchange isn’t too hard — trinket comes in, cash goes out. Every purchase follows this general pattern. With corporate actions, though, there is no pattern and that’s what makes it such an interesting problem.</p><p>Let’s say we get a file with the following data.</p><iframe src="https://cdn.embedly.com/widgets/media.html?src=https%3A%2F%2Fsheetsu.com%2Ftables%2F314f9f75e3&amp;dntp=1&amp;url=https%3A%2F%2Fsheetsu.com%2Ftables%2F314f9f75e3&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;type=text%2Fhtml&amp;schema=sheetsu" width="700" height="386" frameborder="0" scrolling="no"><a href="https://medium.com/media/61d05f8b3a25a8c1d464836d3fd2cc2d/href">https://medium.com/media/61d05f8b3a25a8c1d464836d3fd2cc2d/href</a></iframe><p>Should this be interpreted as the diagram on the left or the one on the right? We know there are corporate actions happening, but the way each is grouped isn’t clear from the file.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*NdP1eORd5gp5Q5ccrta01w.png" /></figure><h3>Automating What We Can</h3><p>Our system is excellent at capturing unit inflow or outflow on individual securities — we rarely get this accounting portion wrong. The situation only becomes more delicate when we start trying to deduce how these flows should be paired. Most financial transactions are well-defined and piecing them together, while a challenging problem, is still something very achievable. For the less defined ones, like corporate actions, it can be very easy to fall into an over-optimization rabbit hole. We’re dealing with a transaction that has an unknown amount of pieces, with each going in an unknown direction at unknown ratios.</p><p>We decided that, instead of trying to force a structure to be defined and applied before it was necessary, these intricate events would instead just be processed as uncoupled basic transactions that make up the event. These transactions capture all of the individual information and unit deltas on each security, doing as much of the work as possible. This is the accounting portion mentioned earlier and it is something that is <em>always</em> automatable.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*htOTBEeoD9JXSPFx9EPmXg.png" /></figure><h3>Trusting Humans to Resolve What Can’t be Automated</h3><p>The only pieces absent now are grouping and direction. We’ve isolated and tackled everything with automation <em>except</em> for these components, which a person can resolve quickly and accurately.<em> </em>We obtain this structure by asking the user two questions.</p><ul><li>Which transactions are part of the same economic event and thus should be grouped together? It’s totally feasible that multiple corporate actions occur on a single day.</li><li>Once they’re in a grouping, where does each transaction logically get inserted?</li></ul><p>Over time, as patterns emerge in these groupings, we acknowledge them, learn from them and introduce new automation, reducing the amount of human intervention necessary. This eventually leaves us with a system where supervision is only necessary in special snowflake scenarios.</p><p>My favorite analogy for this is a railroad. All of the trains are built and zooming along — we just need someone to know which levers to pull so that they all arrive at the correct destination. As more trips take place and our sample size grows, more automatic controls get incorporated.</p><h3>Open Minds and Creative Solutions</h3><p>We’re continually learning lessons at Addepar. With a massive and ever-changing financial world, we have to make sure we are ready to adapt as it evolves around us. Keeping our data models flexible is one way we are enabling ourselves to stay attuned to the financial world. Accepting the fact that not every problem is best solved by only using technology is another philosophy that has proven to be tremendously rewarding. As an engineer at Addepar, a place that defines itself by its tech, this is personally humbling. If we want to tackle and solve the problems that matter, we need to stay as nimble as the ecosystem we’re operating in. Sometimes, this will mean going down unfamiliar and unintuitive paths.</p><p>Despite continuously being hard at work on improving our pipeline, we’re always looking for new ideas. Drop us a note below!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=863d22a0b437" width="1" height="1"><hr><p><a href="https://medium.com/build-addepar/art-of-automation-striking-a-balance-863d22a0b437">Art of Automation: Striking a Balance</a> was originally published in <a href="https://medium.com/build-addepar">Build@Addepar</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[ES Classes in Ember.js]]></title>
            <link>https://medium.com/build-addepar/es-classes-in-ember-js-63e948e9d78e?source=rss----596e43e5e150---4</link>
            <guid isPermaLink="false">https://medium.com/p/63e948e9d78e</guid>
            <category><![CDATA[ember]]></category>
            <category><![CDATA[emberjs]]></category>
            <category><![CDATA[es-classes]]></category>
            <category><![CDATA[es2017]]></category>
            <dc:creator><![CDATA[pzuraq]]></dc:creator>
            <pubDate>Wed, 15 Nov 2017 20:01:01 GMT</pubDate>
            <atom:updated>2018-02-06T04:43:33.105Z</atom:updated>
            <content:encoded><![CDATA[<p>Classes have been an emerging standard in Javascript for some time now. With features like <a href="https://github.com/tc39/proposal-class-fields">class fields</a>, <a href="https://github.com/tc39/proposal-private-methods">private methods and fields</a>, and <a href="https://github.com/tc39/proposal-unified-class-features">decorators</a> on the way, pretty soon Javascript will have a fully featured class syntax with capabilities that haven’t been available in the language before. While Ember.js has had its own, trusty, custom object-model in Ember.CoreObject since the very beginning, these broader advancements are going to be the future of the language and the web itself, so in time we will have to build out and replace the existing system with one based on ES2017+ classes.</p><p>If you’ve been paying attention to Ember RFC process you may have noticed that an <a href="https://github.com/emberjs/rfcs/blob/master/text/0240-es-classes.md">RFC for ES Classes</a> was accepted recently. The RFC was very minor: it didn’t propose any overhauls or breaking changes to the Ember object model as it stands. In fact, it was really just formalizing an existing oddity — ES classes work with Ember as is, right now, as far back as Ember v1.11 (and possibly even farther). You can use them today, along with class fields and decorators, and get the benefits of modern JS syntax, including:</p><ul><li>Better tooling from editors like <a href="https://code.visualstudio.com/">VS Code</a> and <a href="https://www.jetbrains.com/webstorm/">Webstorm</a></li><li>Better static analysis tools and documentation generators, like <a href="https://esdoc.org/">ES Doc</a></li><li>Shared code and solutions with a wider ecosystem, including other decorator libraries such as <a href="https://github.com/jayphelps/core-decorators">core-decorators</a></li><li>Shared knowledge with a wider ecosystem — no more needing to teach the Ember object model to new developers, they just need to know Javascript and they’re good to go</li><li>A simpler, cleaner, declarative syntax that’s easier to understand and makes your code more readable</li></ul><p>Of course, there are some caveats with switching to classes today. Certain Ember features are currently broken and being fixed as-per the RFC. Others, like class fields, have changed dramatically, and you’ll likely need to update your mental model just a bit.</p><p>This guide will lay out all you need to know about using classes in Ember today, including some new techniques that add additional layers of safety and clarity when writing Ember code.</p><p><strong><em>One last note before we get started:</em></strong><em> Most of this guide assumes that you’ll be using Class Fields and Decorators, which are stage 3 and 2, respectively, in the TC39 process. This means that while they definitely have a future in Javascript, they could change as time goes on. (Decorators will already be going through a major overhaul from stage 1 to 2 in Babel 7.) The </em><a href="https://github.com/ember-decorators"><em>ember-decorators</em></a><em> project is dedicated to moving forward as stably as possible, but we can’t control the spec, so Your-Mileage-May-Vary.</em></p><h3>The Basics</h3><p>When you’re ready to start using classes in Ember, the first thing you’ll want to do is install <a href="https://github.com/ember-decorators/ember-decorators">ember-decorators</a>:</p><pre>$ ember install ember-decorators</pre><p>This addon adds babel transforms for decorators and class fields and provides a suite of decorators for common ember functionality. These are not official Ember decorators, and technically most things can be accomplished without them, but it would be overly complicated and verbose so they are highly recommended.</p><p>Let’s start with a minimal component example:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/8162ceafbb92c494484ea5bea9aa0642/href">https://medium.com/media/8162ceafbb92c494484ea5bea9aa0642/href</a></iframe><p>As you can see, they’re very similar. Lifecycle hooks like didInsertElement, didReceiveAttrs, and others still work, along with event hooks like click, hover, etc. For standard classes you should still use create to make instances of the class, and other standard methods should work as expected.</p><p>The biggest difference is that we use the class constructor function instead of init. While the init hook will still work in current Ember versions, constructor should be preferred as a more semantically correct alternative. In addition, init does <em>not</em> work with legacy versions of Ember (more on that later).</p><p>Another major difference is that we can optionally provide a class name to the class keyword. You should always do this to ensure that instances of the class can display the name, which solves one of the oldest problems in Ember — figuring out the name of a class, especially one that isn’t instantiated via the container.</p><p>Finally, there are some major features which are currently broken and will be fixed in the future versions Ember, including:</p><ul><li>Observers and event listeners</li><li>Merged/concatenated properties</li><li>Being able to use .extend to extend ES Classes</li></ul><p>However, nearly everything else has an equivalent alternative through class fields and decorators.</p><h3>Class Fields</h3><p>Class fields are probably the most fundamentally different part of ES classes when compared to the Ember Object model. Here’s a simple example of them that demonstrates the difference:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bbcaa0d1211ea7966d5bb1ae09f59391/href">https://medium.com/media/bbcaa0d1211ea7966d5bb1ae09f59391/href</a></iframe><h4>What’s Going On?</h4><p>Under the hood, EmberObject.extend takes all the values provided to it and places them directly on the <em>prototype</em> of the class. This means that class fields are prototype state, not instance state, and so when we provide a value to the instance via create it gets overridden. This is also what leads to some of the common gotchas surrounding class fields, like how every instance of the class can end up sharing the <em>same</em> instance of an array or object on the class prototype.</p><p>By contrast, ES classes place fields directly on the <em>instance</em> of the class.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*pHkDoEzWw4-4liUTOhJsKQ.png" /></figure><p>This means that every instance will get its own copy of the initial value. This is very useful and intuitive for the cases where we want to ensure state is not shared with other instances, but it means we have to understand the construction of our instances a bit more.</p><p>The example above translates essentially to this:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c4541c82f7d216ae0e8c1bf219d162bf/href">https://medium.com/media/c4541c82f7d216ae0e8c1bf219d162bf/href</a></iframe><p>Classes have no way of modifying their superclass, and following the rules of constructors they must wait for the superclass constructor to be called before touching the instance via this, which in turn means that fields can only be instantiated <em>after </em>the superclass has already set all the values passed into create. It makes sense, but it’s somewhat inconvenient for Ember developers.</p><p>There are a few ways of addressing this:</p><ul><li>Default values can be set in the constructor instead of as class fields.</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/fcc6670e8670f2e829feba7063647944/href">https://medium.com/media/fcc6670e8670f2e829feba7063647944/href</a></iframe><ul><li>Default values can be provided by an initializer. Class fields can be provided an expression which will be run for each instance of the class (like creating a new array or object, for instance).</li></ul><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/157e1c6861665c2bb2995857e41a9c43/href">https://medium.com/media/157e1c6861665c2bb2995857e41a9c43/href</a></iframe><ul><li>Decorators like those provided in the @ember-decorators/argument addon can be used to mark the fields as arguments the object will receive, and set the default if one does not exist. We’ll touch on this approach more later.</li></ul><p>The difference in placement of fields may seem small, but it has pretty large ramifications in how we write code. There are plenty of benefits to this new behavior, but it definitely takes some getting used to, especially for experienced Ember developers.</p><h3>Decorators</h3><p>The <a href="https://github.com/ember-decorators/ember-decorators">ember-decorators</a> addon provides decorators for:</p><ul><li>Computed Properties</li><li>Component Element Customization</li><li>Injections</li><li>Actions</li><li>Observers and Events (Currently broken with ES Classes)</li><li>Ember Data (Currently broken with ES Classes)</li></ul><p>We’ll go over each with a brief example and description of the differences and caveats. For more detailed information, check out the <a href="https://ember-decorators.github.io/ember-decorators/docs/index.html">API docs</a> for the library.</p><h4>Computed Properties</h4><p>The @computed decorator was one of the very first demos of how decorators could be used in Ember. Early examples and the first iteration of ember-decorators, back when it was called ember-computed-decorators, used it directly in the Ember Object model. The option to use decorators on POJOs looks like it may not make it through TC39, but the decorator itself is still around and works beautifully with class syntax:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/50c430565efebf6384176a9484b9967f/href">https://medium.com/media/50c430565efebf6384176a9484b9967f/href</a></iframe><p>As you can see, the decorators use native ES getter/setter syntax instead of plain methods. The syntax is meant to be clearer overall and enforce method parameters, but the properties themselves must still be manipulated with get and set. For computeds which have a setter, the decorator only needs to be applied once to either the getter or setter.</p><p>The readOnly decorator can also be used to mark computeds as read only, instead of the chained method like in the original syntax. volatile computeds which normally recompute their value each time they are accessed can be replaced with a normal, native ES getter, which does this by default.</p><h4>Computed Macros</h4><p>Most of the standard Ember computed macros such as alias, and, or, etc. are available in ember-decorators as well. They can be applied directly to empty class fields:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/37311d69a969519f7376c95e3b67a2fd/href">https://medium.com/media/37311d69a969519f7376c95e3b67a2fd/href</a></iframe><p>The notable exception is readOnly, which was omitted to prevent a collision with the existing @readOnly modifier. The solution here is to modify @alias or @reads with @readOnly:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/aabc712be9c7711d48dfd7d280c8512d/href">https://medium.com/media/aabc712be9c7711d48dfd7d280c8512d/href</a></iframe><h4>Component Element Customization</h4><p>Experienced Ember devs are probably familiar with the tagName, classNames, classNameBindings, and attributeBindings properties that can be used to customize a component’s element. These special properties can be a common source of confusion for new developers, and Ember is trying to move away from them in the near future with Glimmer components, but for the time being they are still necessary.</p><p>tagName is a special property that needs to be applied <em>before</em> the component initializes in some cases, i.e. on the component’s prototype. The other three are examples of concatenated properties — they append their values to the values of property on the superclass — which we pointed out in the beginning are currently broken in ES classes.</p><p>The solution is a combination of class and property decorators:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1069a4358300eb12d6947c55432faff6/href">https://medium.com/media/1069a4358300eb12d6947c55432faff6/href</a></iframe><p>The class decorators allow you to specify properties of the class in advance, while the property decorators allow you to both declaratively specify the binding and the default value in a single statement.</p><h4>Injections</h4><p>The @service and @controller decorators exist to allow you to inject services and controllers into classes. They work very similarly to the existing syntax, the just need to be applied to an empty class field. A service name can be provided to the decorator, or it can infer the name via reflection:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a575ed2b3e5a383d5d57525773bf6300/href">https://medium.com/media/a575ed2b3e5a383d5d57525773bf6300/href</a></iframe><h4>Actions</h4><p>The actions hash on Ember objects is the most common example of a merged property — one whose values will be merged with the actions hash of the superclass, and so on. Similar to the @attribute and @className helpers, ember-decorators provides an @action decorator which can be applied directly to class methods:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/cf625835309dc43892c239483501096e/href">https://medium.com/media/cf625835309dc43892c239483501096e/href</a></iframe><p>One key difference this causes is that the method exists on the class itself in the ES Class version. This means it can conflict with other lifecycle or event hooks, so be aware of name collisions.</p><h4>Observers and Events</h4><p>The @on and @observes decorators will allow you to turn functions into event listeners and observers once the work has been done to fix the issues in Ember.Object. Currently, they don’t work, and one major caveat of them not working is that classes that have existing listeners or observers will <em>not</em> properly override those when being extended — if you have an observer or event listener named foo and you try to override it in a subclass, it will still fire.</p><p>When they are fixed, you’ll be able to use them like so:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/27051040b14fe129f4dc86d926ea1231/href">https://medium.com/media/27051040b14fe129f4dc86d926ea1231/href</a></iframe><h4>Ember Data</h4><p>There are decorators for @attr, @hasMany, and @belongsTo in the ember-decorators library that currently work on the standard Ember Object model using DS.Model.extend, but do not work on ES classes. For the moment, the recommendation is to continue using .extend with Ember Data models.</p><h3>Mixins</h3><p>The last major Ember feature we’ll touch on are Mixins. Mixins are an integral part of Ember, they are the foundation of the Ember Object model (quite literally, they’re core to how extending works!) However, they are leftover from a time when Javascript didn’t <em>have</em> a class system, and each framework had to invent their own.</p><p>Integrating them into ES Classes would require a complete rework of how the the mixin system and Core Object works internally. On top of that, mixins are not an Ember-specific pattern — plenty of other frameworks have implemented them, and more systems will likely emerge as class decorators become standardized. With that in mind, the RFC’s position was that mixins should not be reworked to work with ES class syntax.</p><p>If you still really need them, however, you can continue using .extend to mix them in. When extending has been fixed for ES classes in general they will be usable anywhere:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/80568a0cf7b75e9c010fb178c56df694/href">https://medium.com/media/80568a0cf7b75e9c010fb178c56df694/href</a></iframe><h3>Argument Decorators</h3><p>The <a href="https://github.com/ember-decorators/argument">@ember-decorators/argument</a> addon provides a set of decorators that accomplish two things:</p><ol><li>Provide a sane way to set defaults on components and other objects via the @argument decorator, addressing the issues brought up by the segment on class fields above.</li><li>Provide runtime type and invariant validations, inspired by the excellent <a href="https://github.com/ciena-blueplanet/ember-prop-types">ember-prop-types</a> library. These validations are completely removed from production builds by default, so they are effectively zero-cost.</li></ol><h4>Providing Defaults</h4><p>The @argument decorator marks a field as an argument and sets the default if one hasn’t been set. It takes its name from Glimmer.js, which requires that users make a distinction between arguments and attributes when invoking a component. Arguments get passed into the component, while attributes get applied to the component’s element. The name also implies similarity to a function call, which is a helpful mental model for thinking about components — you are calling them from the template with some arguments, just like a function.</p><p>By default, components will throw an error when you attempt to use them with arguments that haven’t been defined. This does not apply to other types of objects, and it can be turned off via an ember-cli option:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f5c5a36da153ff4c232b04974f61dadc/href">https://medium.com/media/f5c5a36da153ff4c232b04974f61dadc/href</a></iframe><p>Fields marked with the @attribute and @className decorators are also whitelisted, so they won’t throw errors.</p><h4>Specifying Validations</h4><p>You can use the @type, @required, and @immutable decorators to specify invariants about various fields, arguments, and attributes. The validations run once at the end of object creation, and in the case of @type and @immutable whenever you attempt to set their values.</p><p>Types can either be a string representing a primitive type, a class that the field is an instance of, or a type made using one of the type helpers: unionOf, arrayOf, or shapeOf. Some predefined types are included with the library, including:</p><ul><li>Action: Union type of string and Function , and recommended for specifying actions that are passed into components</li><li>ClosureAction: Alias of Function and recommended if you want to enforce only closure actions</li><li>Element: Fastboot-safe alias for window.Element</li><li>Node: Fastboot-safe alias for window.Node</li></ul><p>Here’s an example showcasing the flexibility of these decorators:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/247ebb926c847e7cf4f1476bb609d9d3/href">https://medium.com/media/247ebb926c847e7cf4f1476bb609d9d3/href</a></iframe><p>All of these extra validations are stripped from production builds by default, so you won’t have to worry about them impacting the performance of your app. For more detailed usage docs, checkout <a href="https://github.com/ember-decorators/argument">the documentation</a>.</p><h3>Legacy Usage</h3><p>Prior to Ember v2.13, the framework accomplished dependency injections by extending classes a second time and adding the injections. This breaks the ES class constructor function, which in turn breaks class fields.</p><p>If you’re on an older version of Ember, you can install the <a href="https://github.com/pzuraq/ember-legacy-class-shim">ember-legacy-class-shim</a>:</p><pre>$ ember install ember-legacy-class-shim</pre><p>This addon reopens Ember.Object to change the behavior of the extend function when being used on native classes for injection. This does <em>not </em>fix extend for general usage on native classes however, as that requires changes to Ember.Object in the Ember.js core.</p><h3>What Comes Next</h3><p>Now that you know how to use ES Classes in your app, you may be curious about what’s coming up next with the evolving spec!</p><p>As I noted in the beginning, upgrading to Babel 7 and the latest version of the spec should be interesting, but ember-decorators and @ember-decorators/argument should be able maintain their existing APIs. Fixes for the broken functionality like observers and events are in the works in Ember core, and fixes for the Ember Data decorators should come soon.</p><p>There are also some projects in the works to take advantage of the declarative and standardized nature of this syntax to work on better tooling for Ember users in general. The type information provided by @ember-decorators/argument can be used to automatically generate thorough component documentation, including both the arguments each component receives and the actions it sends. At some point the metadata may be usable in better static analysis tools as well (although at that point it may be better to switch to ember-cli-typescript)!</p><p>Eventually, as decorators are finalized and accepted into Javascript, RFCs will eventually land in Ember itself for an official set of decorators. The ember-decorators project will likely continue past that, deprecating decorators that are replaced but continuing to support supplemental decorators like those in @ember-decorators/argument.</p><h3>Putting It All Together</h3><p>To summarize the new API, here’s an attempt at a not-totally-contrived example component that demonstrates the differences:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3ca52b699175cda8ae161705e18ec078/href">https://medium.com/media/3ca52b699175cda8ae161705e18ec078/href</a></iframe><p>Overall the result is clearer and easier to read, the decorators provide context and are self-documenting, and the final component has more levels of safety than before.</p><p>In review:</p><ul><li>ES Classes can be used with Ember today, as far back as Ember v1.11</li><li>Major differences include:<br>- constructor replaces init<br>- Class fields are applied to the instance, not the prototype<br>- Decorators should be used for most Ember functionality, like computeds and service injection</li><li>Major caveats include:<br>- Observers and events do not work<br>- Merged and concatenated properties do not work<br>- .extend does not work on ES Classes themselves<br>- Mixins cannot be applied to ES Classes, you must use .extend</li><li>If you’re using a version of Ember under 2.13, you should install <a href="https://github.com/pzuraq/ember-legacy-class-transform">ember-legacy-class-transform</a> addon</li><li>Checkout <a href="https://github.com/ember-decorators/argument">@ember-decorators/argument</a> for helpful, declarative validations and sane defaults</li><li>More class-y goodness is in the pipeline!</li></ul><p>Thanks for reading, and if you’re curious about the project, would like to ask questions, or wanna help out, check out the #topic-es-decorators channel on the <a href="https://ember-community-slackin.herokuapp.com/">Ember Community Slack</a>!</p><p><em>If you’re looking to build ambitious apps with Ember.js and the latest in ES2017+ standards, </em><a href="https://addepar.com/careers/"><em>Addepar is hiring</em></a><em>!</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=63e948e9d78e" width="1" height="1"><hr><p><a href="https://medium.com/build-addepar/es-classes-in-ember-js-63e948e9d78e">ES Classes in Ember.js</a> was originally published in <a href="https://medium.com/build-addepar">Build@Addepar</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Retrofitting Position IDs to Addepar Non-Intrusively]]></title>
            <link>https://medium.com/build-addepar/retrofitting-position-ids-to-addepar-non-intrusively-398126f874a9?source=rss----596e43e5e150---4</link>
            <guid isPermaLink="false">https://medium.com/p/398126f874a9</guid>
            <category><![CDATA[edge]]></category>
            <category><![CDATA[node]]></category>
            <category><![CDATA[fintech]]></category>
            <category><![CDATA[reconciliation]]></category>
            <category><![CDATA[graph]]></category>
            <dc:creator><![CDATA[Calvin Wu]]></dc:creator>
            <pubDate>Wed, 08 Nov 2017 14:11:00 GMT</pubDate>
            <atom:updated>2017-11-08T20:05:40.370Z</atom:updated>
            <content:encoded><![CDATA[<p>Every day, Addepar’s data pipeline consumes portfolio data from hundreds of different custodians and imports it to millions of nodes and edges in our <a href="https://medium.com/build-addepar/the-financial-graph-2ba9b9321754">Financial Graph</a>. To ensure that the data can be trusted and used to correctly calculate performance, it’s crucial that we not only import it in a timely fashion, but also run verification checks with what already exists on our platform. The data is complex, and data integrity check failures can be caused by a lot of different issues — for example, missing data, incorrect raw data from custodian, incorrect transaction mapping, problematic validation logic, and incorrect security mapping.</p><p>As our platform grows, we’re continually improving and extending our methods. The following is an example of how we addressed one type of verification problem — node matching — by non-intrusively introducing a core change to a critical pipeline.</p><h3>Node Matching</h3><p>The <a href="https://medium.com/build-addepar/the-financial-graph-2ba9b9321754">Financial Graph</a> is used to represent the ownership structure of portfolio data, where a <em>node </em>represents an entity (such as a brokerage account, an asset owner, a legal entity, a stock, or a bond), and<em> </em>an<em> edge</em> (or a <em>position </em>in financial terms) represents the ownership relationship between two nodes. The <em>from</em> node is often an account, and the <em>to </em>node refers to the portfolio’s holding — the actual security.</p><p>When importing data, we need to do a few things:</p><ol><li>Identify the account node.</li><li>Identify the security node via <em>node matching. </em>We search for the security node in our system using the security attribute information the custodian provided. The search criteria could be the name of the security or security identifiers. Each security type may have different preferred strong and weak identifiers that drive the matching. Our matching logic may evolve over time as we learn more about the data. This is similar to the entity resolution problem, where we have to identify the same node across different data sources.</li><li>Find the edge. Once both the account node and security node are identified, the edge between them is where daily transactions and position snapshots are stored.</li></ol><p>Ideally, the security nodes are unique and shared between the account nodes. After all, the edge is what references the client’s specific holding.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*n_2QaxdJ7VH-cTG_yAwb1Q.png" /><figcaption>Financial Graph representation of a portfolio.</figcaption></figure><h3>The Node Jumping Problem</h3><p>Addepar’s platform imports daily account portfolio data from hundreds of different data sources. Each source may represent each security slightly differently. When there isn’t enough information in the feed to identify the corresponding node for the security, the system treats it as a new security and automatically creates a new node. Clients may also edit the security attributes directly, which can also cause the system to create a new node. Over time, multiple nodes may represent the same security.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*ZVFsP__cF4LLGeWezqcRyQ.png" /><figcaption>Accounts referencing duplicate security nodes.</figcaption></figure><p>Having duplicate nodes isn’t great, but it’s not the end of world. It has no impact on the accurate representation of the client’s portfolio. What’s important here is the <em>edge, </em>where the position snapshots and transactions are stored<em>. </em>We need to be able to import data to the right edge all the time. The real problem with duplicate nodes is that it increases the probability of the system matching the account’s holding to the wrong security node, hence breaking the time series. This in turn causes unit verification failures and performance calculation errors.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nC517c3IcpCWZdNUqL0MgA.gif" /><figcaption>A Node Jump for Account I on Day 4</figcaption></figure><p>Above is an example showing that Account I’s Facebook holding has time series data for three days. However, on Day 4 after the import, it only has one day of position data because the wrong security node was identified, which created a new edge. This causes the time series to drop off in what we refer to internally as the <em>Node Jumping</em> problem.</p><h3>Finding the Right Edge</h3><p>Instead of searching the <em>from </em>and <em>to </em>nodes first and identifying the edge, what if the system could just match with the edge directly? The benefit here is that matching becomes very straightforward, and node matching is bypassed for the most part, minimizing the chance of node jumping. The challenge is that we’re introducing a new primary key that bridges two large internal systems. It’s a fundamental change to the data flow of Addepar’s daily portfolio data imports. The adoption risk is very high. We took this challenge into account when deciding how to design and deliver this change.</p><p>At a high level, the new workflow leveraging the new Position ID, goes like this:</p><ol><li>Every position from each feed has a unique Position ID. Ideally, this ID is the same over time.</li><li>The system uses this ID to find the corresponding edge.</li><li>If found, that’s where the data is imported.</li><li>If not found, the system falls back to the existing workflow of node matching and updates the edge with a reference to this Position ID, so future imports can leverage it.</li></ol><h3>Position ID</h3><p>The Position ID is the unique identifier for each holding in a portfolio. Some portfolio data partners have this concept for us to leverage. Others require examining patterns in the feed data to come up with the best derived key. While we may know what combinations make up the best Position ID, the assumption still needs to be validated for the entire time series. The goal is to find the most stable combination in the time series. Position ID loses its meaning when it mutates too often because the process will just fall back to node matching. The combination that mutates the least over time is considered the strongest ID.</p><h3>Temporal Design</h3><p>Ideally, Position ID is a constant value like a true primary key. In reality, Position ID mutates over time as security attribute changes. This could happen for a variety of reasons, the primary driver being data coming from the custodians or corporate actions. To accommodate this requirement, Position ID persistence utilizes a temporal model where each ID has a <em>start </em>and <em>end </em>date to track its validity. This is essential for historical imports and replaying imports: as IDs change over time, the ability to hit the right edge each time is crucial. Each Position ID should only be assigned to one edge only. There should never be a case where a Position ID points to more than one edge. To ensure data integrity, any conflicting ID assignment is invalidated with every insert.</p><p>The temporal model also provides a complete history of the Position ID (PID) assignment of an edge. This is particularly useful when debugging any mapping abnormalities.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*miv-EOIoExFKjokp88GU_A.png" /><figcaption>Time series of a Position ID.</figcaption></figure><h3>Position ID Release</h3><p>There is a big difference between building a system from scratch and strategically evolving a running system. With the former, you have fewer constraints on the design, no concerns about backward compatibility, and, most importantly, no possibility of impacting a working solution. Often times, we have to deal with the latter, making core changes to a critical pipeline, whether that’s migrating a database, adopting a new messaging infrastructure, or refactoring a large interface. When we do, we need to constantly think about the risk with each release and how we can leave the production system better than we found it.</p><p>For the Position ID release, the known and unknown risks were very high because the change could directly impact the integrity of the client’s portfolio. Before we could roll out this feature, we first needed to understand the worst case scenarios:</p><ul><li>Unexpected exceptions in the new flow causing imports to fail.</li><li>Weak Position IDs causing node jumping to appear again. If a Position ID changes frequently, matching can’t depend on the Position ID reliably, so it falls back to the existing node matching. The worst case scenario here is node jumping, which we know how to deal with already.</li><li>Buggy Position ID matching, which could cause massive data integrity check failures and post data clean up effort. This is a high-risk unknown. We would want a way to revert the change quickly and keep the impact to a minimum.</li><li>Other Unknowns. We need a way to revert quickly in case of an unknown failure that’s blocking data import.</li></ul><p>Thorough unit test coverage and feature flagging goes a long way toward ensuring the quality of a release. Another important aspect here is test coverage. How we do ensure the test data set is meaningful and covers all the edge cases? The following captures the steps we took for this feature rollout:</p><ul><li>Instead of a binary feature flag, we used multifaceted feature flags to give us fine-grained controls on when and how widely we release and to allow us to rollback with minimal impact.</li><li>We upgraded our staging environment to mirror production. In addition, we could replay production traffic in staging. This allowed us to test in full scale and have production quality test coverage.</li><li>We rolled out persisting Position IDs without using them for matching. This gave us a critical view on the quality of the Position ID chosen and allowed us to learn from this data. We’ve learned what Position ID combination worked well and what needed adjustment.</li><li>We rolled out Position ID matching by client. We started out with smaller clients or clients with less volatile instrument attributes. This greatly reduced the risk of the unknowns.</li><li>We gradually increased the roll out size with each release, and learned from any bugs that surfaced. This allowed us to keep the impact to a minimum.</li><li>We worked closely with our Data Operations team throughout the process. They work with this data day in and day out. Their knowledge of the data and feedback throughout this process drove the quality of the migration.</li></ul><p>Releasing this feature was a lengthy process and it took a few iterations but with these careful measures, the release went out without any issues and we have been using Position IDs successfully to improve our data quality.</p><h3>What’s Next</h3><p>Our data footprint is growing exponentially. Continually investing in, retrofitting, and upgrading our platform is a constant theme at Addepar. We balance these challenges with careful planning around known and unknown risks, always look to learn from our mistakes, and collaborate across teams of passionate engineers who enjoy solving tough problems. There will be more blogs in this space around large scale data migration, import and performance challenges. Stay tuned!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=398126f874a9" width="1" height="1"><hr><p><a href="https://medium.com/build-addepar/retrofitting-position-ids-to-addepar-non-intrusively-398126f874a9">Retrofitting Position IDs to Addepar Non-Intrusively</a> was originally published in <a href="https://medium.com/build-addepar">Build@Addepar</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Addepar Form Studies]]></title>
            <link>https://medium.com/build-addepar/addepar-form-studies-738bed7a003b?source=rss----596e43e5e150---4</link>
            <guid isPermaLink="false">https://medium.com/p/738bed7a003b</guid>
            <category><![CDATA[forms]]></category>
            <category><![CDATA[design]]></category>
            <dc:creator><![CDATA[Nick D'Amico]]></dc:creator>
            <pubDate>Tue, 31 Oct 2017 18:00:17 GMT</pubDate>
            <atom:updated>2017-10-31T18:00:17.270Z</atom:updated>
            <content:encoded><![CDATA[<h4>Exploring Identity Through Line, Shape, &amp; Composition</h4><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*4RKLoDnV3b3idLNHqUgKTQ.jpeg" /></figure><p>Guided by a traditional approach to visual research and identity development, the Addepar Design team performed a series of studies to explore how we could translate our logo into a compelling, consistent visual language.</p><p>We started by breaking the mark into its essential elements — shapes, lines, angles, and its general nature. Next, we created compositions under systematic restrictions so that we could observe the effects of each variation. We began rounds of study by sketching independently, then regrouped to critique the results and redefine our parameters for the next layer of exploration.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*cSwLgB9F5AdTz-1yKri_YA.jpeg" /></figure><p>Ultimately, we were able to extend Addepar’s logo into a collection of unique compositions under the same visual language. We’ve been able to repurpose our work to suit many different demands of medium and emotion.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*5k_hy6F8Ywkb2W8Y-cdCOw.jpeg" /></figure><p>The result of this exploration is our book,<em> Addepar Form Studies</em>, that gathers a few of our best designs and their applications. We discuss our process, methods, and interpretations to provide readers insight into the way we work.</p><p>Read the full story on our team’s <a href="https://www.behance.net/addeparcreative">Behance page</a>.</p><p><a href="https://20207-presscdn-pagely.netdna-ssl.com/wp-content/uploads/2016/10/Form-Studies-book-READER-1.pdf">You can download the PDF book here.</a></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=738bed7a003b" width="1" height="1"><hr><p><a href="https://medium.com/build-addepar/addepar-form-studies-738bed7a003b">Addepar Form Studies</a> was originally published in <a href="https://medium.com/build-addepar">Build@Addepar</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Migrating Mountains of Mongo Data]]></title>
            <link>https://medium.com/build-addepar/migrating-mountains-of-mongo-data-63e530539952?source=rss----596e43e5e150---4</link>
            <guid isPermaLink="false">https://medium.com/p/63e530539952</guid>
            <category><![CDATA[mongodb]]></category>
            <category><![CDATA[migration]]></category>
            <category><![CDATA[database]]></category>
            <dc:creator><![CDATA[Elan Kugelmass]]></dc:creator>
            <pubDate>Tue, 24 Oct 2017 13:11:09 GMT</pubDate>
            <atom:updated>2017-11-08T00:25:49.998Z</atom:updated>
            <content:encoded><![CDATA[<p>At Addepar, we’re building the world’s most versatile financial analytics engine. To feed the calculations that give our clients an unprecedented view into their portfolios, we need data — from as many sources, vendors, and intermediaries as possible. Our market and portfolio data pipelines ingest benchmarks, security terms, accounting, and performance data from hundreds of integration partners. Behind this data pipeline is a database. And like every database, ours requires maintenance and care.</p><p>Maintaining an obsolete database instance is challenging due to lack of support, inferior performance, and a dwindling developer community. As our dataset grew and we faced increased stability and performance requirements, the engineering group at Addepar decided it was time to upgrade our venerable <a href="https://www.percona.com/software/mongo-database/percona-tokumx">Mongo 2.4 (TokuMX 2.0)</a> database to the latest and greatest Mongo 3.4. Every organization has at least one database saga, and we’re excited to share one of ours.</p><h3>Dependency management extends to databases</h3><p>Upgrading a database is a tricky business. Like other dependencies that support a product, databases have a tendency to fall out of date. Upgrades are deferred until that imagined future where everything is stable, clients have exhausted their feature request lists, and there’s not much to do in the office other than play foosball and exchange memes. It’s an understandable decision! Databases are complicated, leaky abstractions that inevitably form an implicit extension of our application logic. Their data types, atomicity guarantees, and transactionality semantics define the constraints we place on data and drive how we store application state. And because these guarantees (or lack thereof) tend to change (in ways that are sometimes undocumented!) between database versions, moving to the latest release is a risky proposition.</p><p>Motivated by our need for an extremely reliable datastore that could handle complex and evolving schemas, we chose Mongo as the <a href="https://medium.com/build-addepar/our-tech-stack-a4f55dab4b0d">sole database for Addepar’s data ingestion pipeline</a> five years ago. We use Mongo not only as <a href="https://www.mongodb.com/document-databases">a document store</a> for persisting state, but also as a <a href="https://docs.mongodb.com/manual/core/gridfs/">blob-store</a> for capturing intermediate, heterogenous results from the pipeline. As each transformation stage in the ETL pipeline succeeds, its result is decorated with indexable metadata and placed in Mongo. This use-case does not require transaction isolation but benefits from high write volumes, flexible schema definition, multiple indices, easy replication, and automatic failover. However, when we initially decided to use Mongo, the database <a href="http://www.arborian.com/2016/03/11/mongodb-mmapv1-wiredtiger-and-queues/">only supported very coarse locks at the collection (i.e. table) level</a>, limiting write throughput. We were not the only ones frustrated by Mongo’s limited support for concurrent operations. TokuMX, an open-source fork with MVCC semantics and index and document level locking, had been developed to fill some of Mongo’s gaps and we were happy to adopt it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*BY7ujPL276ys0LGPyWXvUg.png" /></figure><p>As our daily data intake grew, it became clear that TokuMX, long since deprecated by its new parent company, was no longer meeting our needs. Conflicting access patterns necessitated indices that were less than optimal for most queries. TokuMX’s <a href="https://www.percona.com/blog/2013/09/18/lock-diagnostics-and-index-usage-statistics-in-tokumx-v1-2-1/">index range locks would freeze large chunks of our collections</a>, blocking critical jobs and yielding a worrisome number of operation timeouts. These performance issues made it even more critical that we be able to explore query plans and identify bottlenecks, but diagnostic tools for inspecting the database engine were limited in early versions of the software. Nobody enjoyed wondering why certain operations would sometimes take ten times longer than usual. We needed a new database.</p><p>Fortunately, since our adoption of TokuMX, the mainline Mongo distribution had made impressive gains. The newest version of the database offered a much-improved replacement database engine with optimistic document-level locking and radically improved support for server-side aggregation and transformation of documents. I was thrilled to have access to more bulk-write options that could improve the performance of some of our most “chatty” database operations. Upgrading would resolve some of our performance concerns in the short term, resolve bugs that had long-since been patched in newer versions of the software, and allow us to continue scaling with our clients.</p><h3>Planning the great migration</h3><p>The decision made, we got started with preparation for our great migration. We needed to be sure that our highly-parallel data ingestion pipeline would continue to operate correctly with the new database. We approached the problem from several directions. To catch low-hanging fruit, we beefed up our existing unit tests around our application-level data access abstractions and checked for compatibility with the new database version. To verify expected behavior of the entire pipeline-database system at a small scale, we repurposed our existing integration test suite, which provides very broad coverage of our data pipeline and utilizes multiple concurrent clients.</p><p>We also decided to run both TokuMX and Mongo 3.4 in parallel in a production-like environment. Using Java’s convenient <a href="https://docs.oracle.com/javase/8/docs/technotes/guides/reflection/proxy.html">dynamic proxy</a> support, we transparently intercepted each application-level database request, performed the operation against both databases, and compared both the document results and operation times using a separate thread pool so as not to block the application threads on additional computation. With the help of a few scripts and our log aggregation provider, Sumo Logic, we built a detailed view of the results (correctness) and performance characteristics (latency) of all of our database operations, broken down by service, data provider, and other attributes. Pairwise comparison of all requests showed that Mongo 3.4’s operations were on average faster, with less variance in their duration. While (expected and infrequent) transient failures complicated this simple approach, we could confidently proceed with the changeover knowing that the new database would perform at least as well as the old.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*d_MsJXDxpsuuHVbgQEt-AQ.png" /></figure><p>The portion of our product reliant on the Mongo database is not client facing or driven by external requests. We were able to take advantage of a weekend maintenance period to migrate our database offline. TokuMX’s on-disk format is incompatible with Mongo 3.4’s, ruling out an in-place swap. While it is possible to use a combination of database snapshots and the Mongo family’s “oplog” (akin to the binlog for our MySQL friends) to perform the migration with only several minutes of downtime, looser business requirement allowed us to use the simpler offline process. Still, we needed to be sure that our database would be back up and running before the end of the weekend. Without the database there would be no pipeline, and without the pipeline, our clients would not have their data in time for Monday’s market open.</p><h3>Rehearsal makes execution all that much easier</h3><p>We set up a production-like environment with a fresh Mongo 3.4 cluster, loaded data into that environment, and rehearsed the entire migration procedure several times. This let us experiment with different settings for extracting the data from TokuMX to serialized <a href="http://bsonspec.org/">BSON</a> and re-importing it to Mongo. Tweaking the import configuration for our environment shaved several hours off the whole procedure. (We found that turning off the journal, <a href="https://docs.mongodb.com/manual/reference/program/mongorestore/#cmdoption-noindexrestore">disabling index restoration</a>, and deferring the creation of the replication set until after the data had been restored reduced time to completion by half.) Careful rehearsal also helped us weed out surprises that would have been quite unwelcome during the production migration. Writing out precisely how the collection index specifications needed to be adjusted for Mongo 3.4 (the primary index is now implied to be unique and the <a href="https://www.percona.com/blog/2013/10/11/introducing-tokumx-clustering-indexes-for-mongodb/">concept of clustering</a> no longer applies) was as important as determining how long it would take to move data between machines and which service users would be authorized to take each action.</p><p>When the migration weekend rolled around, the collaboration between our software engineering and DevOps teams paid off. We rolled up our prep work into a detailed script with expected run-times and pre-considered contingencies for possible hiccups. The migration went off with only a few bumps (remember to adjust your configuration management tool to avoid having services unexpectedly restart!). We started processing data again with time to spare until the next business day and with confidence that everything would proceed smoothly. We retired a deprecated, yet critical, dependency, strengthened the relationship between two of our teams, and set the stage for further performance improvements for our clients.</p><p>Our upgrade work set the stage for us to server larger clients more reliably than we could before. Features in the latest Mongo distributions are allowing us to tighten up our operational standards while also evolving our models to help our clients make the most of their financial data. We’re looking forward to persisting our next billion documents, recording data on hundreds of billions of dollars of assets, even faster and more reliably than ever!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=63e530539952" width="1" height="1"><hr><p><a href="https://medium.com/build-addepar/migrating-mountains-of-mongo-data-63e530539952">Migrating Mountains of Mongo Data</a> was originally published in <a href="https://medium.com/build-addepar">Build@Addepar</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A common data format]]></title>
            <link>https://medium.com/build-addepar/a-common-data-format-e3e4b684e47d?source=rss----596e43e5e150---4</link>
            <guid isPermaLink="false">https://medium.com/p/e3e4b684e47d</guid>
            <category><![CDATA[etl]]></category>
            <category><![CDATA[data]]></category>
            <category><![CDATA[fintech]]></category>
            <dc:creator><![CDATA[Dénes Findrik]]></dc:creator>
            <pubDate>Tue, 17 Oct 2017 13:11:06 GMT</pubDate>
            <atom:updated>2017-10-17T13:11:06.841Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7MMIbIuIbwc6xgVd3PHx7Q.png" /></figure><p>Every day we parse financial data from hundreds of sources, including custodial banks, prime brokerages, fund administrators, private banks, and market data vendors. We receive data in a variety of formats: CSVs, TSVs, text files, custom formats — and there are basically as many representations as sources.</p><p>This variety of data formats is a side effect of Finance’s long history with Tech: the industry has been a major force of innovation. Financial institutions have invested a lot to ensure that they have the technology they need to operate. As a result, competing complex systems are now used to represent the same data. Each different system introduces different constraints, and each constraint can have implications for how people think about the inner workings of the financial world.</p><p>How can we push all this data into a consistent <a href="https://medium.com/build-addepar/the-financial-graph-2ba9b9321754">Financial Graph</a>? The answer lies in defining a simple common format.</p><h3>The Problem</h3><p>Receiving data in a lot of different formats isn’t necessarily a hard problem on its own: the greater challenge is handling the significant differences in how the same data is represented from source to source.</p><p>To better illustrate this problem, let’s take a very simple example. We have a single account that already has 100 shares of Alphabet and $15,000 cash. From that account, we buy 20 shares of Apple at $140 per share with a $50 broker fee for the transaction.</p><p>The following are just a few hypothetical ways this same, simple set of transactions could be represented at the end of the day:</p><h4>Source 1</h4><p>Positions.csv</p><pre>Date    ,Account,Symbol,Name      ,Quantity,CurrCode  ,Type,Exchange<br>20170131,1234   ,GOOG  ,Google Inc,100     ,USD       ,STK ,NASDAQ<br>20170131,1234   ,USD   ,US Dollars,12150   ,USD       ,CRY ,<br>20170131,1234   ,AAPL  ,Apple Inc ,20      ,USD       ,STK ,NASDAQ</pre><p>Transactions.csv</p><pre>Date    ,Account,Quantity,Amount,Symbol,CurrencyCode,Type,SettleDate<br>20170131,1234   ,20      ,2800  ,AAPL  ,USD         ,BY  ,20170131<br>20170131,1234   ,0       ,50    ,AAPL  ,USD         ,FEE ,20170131</pre><h4>Source 2</h4><p>Positions.txt</p><pre>Account        Date      Symbol    Quantity        <br>1234           20170131  GOOG      100<br>1234           20170131  USD       12150<br>1234           20170131  AAPL      20</pre><p>Transactions.txt</p><pre>Date      Account        Symbol    Type  Quantity  Price Fees<br>20170131  1234           AAPL      BUY   20        140   50</pre><p>Securities.txt</p><pre>Symbol    Name                    CC  Cat     Date    Price<br>GOOG      Alphabet                USD STOCK   013117  830<br>AAPL      Apple                   USD STOCK   013117  140<br>USD       Cash                    USD CASH    013117  1</pre><h4>Source 3</h4><p>Input.file</p><pre>0,<br>1,2017-Jan-31,1234,<br>2,<br>11,20,AAPL,EQ,<br>11,100,GOOG,EQ,<br>11,12150,USD,CY<br>3,<br>4,<br>23,20,AAPL,BY,<br>24,2800,USD,BY,<br>25,50,USD,FEE,<br>5,<br>99</pre><p>As you can see, the files are completely different not only in their format, but also in the way they capture what happened on this day. While we could create mappers to parse this data and directly push it into the Financial Graph, doing so would limit us in our goal to become a platform for the financial world. Anyone who wanted to build applications on top of Addepar would need a deep knowledge of the inner workings of the graph; it would be harder to ensure that the data pushed directly into the platform met the same requirements as data processed by us; and you would need to test on ‘live data’.</p><p>Avoiding direct mappers is a challenge, but ultimately also offers an opportunity to standardize the data format for the interchange of accounting data.</p><p>By separating out common concepts from the process we use to transform the data, we can validate the data before we try to fit it directly into the graph. We divide these processes to keep our interfaces to the external world as simple as possible while taking in a wide breadth of inputs. Our internal data teams also gain efficiency by dividing ownership between 1) translating the incoming data, and 2) extending our models to new financial entities and transactions.</p><h3>Our Current Solution</h3><h4>Initial Considerations</h4><p>The nature of the financial data we receive dictated our initial, structural decisions.</p><p>First of all, we have to account for many fields that might appear in only one source leading to our data being very sparse. When handling sparse data, you generally want to store it in a dynamic way having an entry only for the fields that are populated. Besides, if you use pre-determined columns then you have to account for the union of all possible attributes of the data types that you’re trying to capture and you might end up with hundreds of columns or have complex logic to use a single column for multiple purposes.</p><p>Additionally, we had to find a level of abstraction general enough to correctly model the vast majority of common financial situations but also flexible enough to allow us to capture the nuances in the ways financial institutions differentiate their data. We looked at existing solutions, such as <a href="http://www.ofx.net/">OFX</a> or <a href="https://www.swift.com/">SWIFT</a>, but they didn’t allow us to capture the variety in the financial data that we see on a daily basis.</p><p>Finally, we wanted to reduce the cognitive load required to inspect the data. Storing different kinds of data in separate files then switching between them or filtering for the right account introduces a lot of context switching when answering even simple questions. We want to have all the relevant data for a single portfolio in one place.</p><h4>Format</h4><p>To accommodate the above considerations, we arrived at a JSON-based representation where we differentiate three main sections: positions, transactions, and securities. It allows us to have an object-oriented representation, solves the problem of sparsity by only populating fields that apply, and allows all the information for a given day to be stored in a single file.</p><pre>{<br>    “date”: “2017-01-31”,<br>    “accounts”: [{<br>        “account_id”:”123456789”,<br>        “positions”:[{...}],<br>        “transactions”:[{...}]<br>    }],<br>    “securities”: [{...}],<br>    “source”: {<br>        “type”: “custodian”,<br>        “details”: “myfavoritecustodian”<br>    }<br>}</pre><p>In the first section, we store all the account-specific end of day position information (i.e. what the custodian tells us you have in your investment account at closing). We only store basic information here such as the security identifier, units held, or the market value of the position.</p><p>Daily transactions are stored in the transactions section. We can easily capture the variety in different transaction types and only store the information relevant to each. For example, a forex transaction would require specifying two currencies as well as the two cash accounts that were involved while in most cases populating a second currency or cash account field is unnecessary. We keep this section lean by only allowing transaction-specific information here and linking that to other relevant information through internally consistent identifiers.</p><p>Finally, in the securities section we store information about all the entities that appear in the positions or transactions and link them through an internally consistent ID. We populate as many attributes as we can capture from the source here, such as security type, name, currency code, ISIN, SEDOL, ticker symbol, identifier for the underlying security for derivatives, etc.</p><p>Of course, using a JSON-based format has its drawbacks: in general, people are less used to thinking in tree-based structures than tabular representations. Most people are familiar with Excel, and many people in the field know how to query a MySQL database even if they don’t have a computer science background. Using a compatible format, we could easily concatenate multiple rows to search across a large date range; however, there are fewer programs that allow you to easily view and explore across many JSONs.</p><h4>Validation</h4><p>To ensure that the data in every file is correct before pushing it into the Financial Graph, we perform a number of validation steps. Using Java and an object-oriented representation guarantees that the right type of data ends up in each field, but we also want to make sure that we don’t accidentally populate certain fields for various types that make no sense. For example, having an option exercise transaction on a cash account would raise quite a few eyebrows and result in many errors further along the road. Similarly, a treasury bond with a maturity date more than 30 years away is assumed to be incorrect; however, this logic can be easily adjusted in our model if longer maturities are introduced by the Department of Treasury.</p><p>Additionally, we validate that the data is complete by making sure that every security identifier that appears in a position or a transaction has a corresponding entry in the securities section. We also double check that we’re not missing any necessary information for any entries. For example, cash accounts would always need an associated currency, bonds need a maturity date, or options need an exercise date.</p><h3>Evolution</h3><p>This data format helps us process hundreds of thousands of accounts every day efficiently and provides a simple common language across all our sources. It is also a work in progress and we hire the best people in the industry to expand our format to more data types and ensure that our data is represented accurately. For example, we’ve recently added the ability to capture custom security attributes and support for representing performance summary data for various securities.</p><p>We’re also working to represent transactions as transaction effects to match <a href="https://www.google.com/patents/US9105064">how they are represented in our Financial Graph</a>. For example, buying 50 shares of Apple for $7,000 would cause two transaction effects: one on Apple to increase our number of shares by 50 and another on our USD cash account to decrease it by 7,000. Using this approach allows us to model even more complex transactions than what our current format allows. Right now, a three-way split with cash effects is generally represented in 3–7 different transactions (depending on the source) — imagine being able to capture all those legs in a single transaction!</p><p>We’re working hard on improving our data format to capture information more granularly while keeping its accuracy and flexibility. By sharing our work, we hope to start a discussion and work towards widespread adoption of the format — if you want to be part of the conversation, leave a comment or email us at <a href="mailto:build@addepar.com">build@addepar.com</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=e3e4b684e47d" width="1" height="1"><hr><p><a href="https://medium.com/build-addepar/a-common-data-format-e3e4b684e47d">A common data format</a> was originally published in <a href="https://medium.com/build-addepar">Build@Addepar</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How We Interview at Addepar]]></title>
            <link>https://medium.com/build-addepar/how-we-interview-at-addepar-9ef77b6bd11b?source=rss----596e43e5e150---4</link>
            <guid isPermaLink="false">https://medium.com/p/9ef77b6bd11b</guid>
            <category><![CDATA[hiring]]></category>
            <category><![CDATA[interview]]></category>
            <dc:creator><![CDATA[Ben]]></dc:creator>
            <pubDate>Tue, 10 Oct 2017 13:16:45 GMT</pubDate>
            <atom:updated>2017-10-10T13:16:45.499Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*v2thEDykDDbm0sT2xxqFGA.jpeg" /></figure><p>At Addepar, we’re committed to cracking the hard problem of technical interviewing. We know that to hire the most qualified, best matched candidates, our interview process needs to be engaging, challenging, and fair.</p><p>When we’re designing our interview process, we ask:</p><ul><li>How do we gather the right information effectively and without bias?</li><li>How do we get the best odds that both Addepar and the candidate will make the right decision coming out of the phone screen or interview day?</li><li>How do we make tradeoffs between how much time we ask current employees to devote to the process and ensuring that candidates have the best possible experience?</li><li>The goal of an interview is to make a hire/no-hire decision and for the candidate to make a decision on whether to join. Compared to decisions like how to design or build a certain feature, these decisions are infrequent. How do we update the process to incorporate learnings as fast possible?</li></ul><p>Seeking and sharing knowledge are among our core company values, and because of that we’re always iterating on the process to incorporate learnings we’ve gained while interviewing hundreds of candidates. So if you’ve interviewed before, or if you’d like to interview again, chances are that the experience will be different the second or third time around. We’ve tested and changed everything from the collaborative document editor we use (Collabedit vs. Stypi vs. HackerRank) to the questions themselves (tracked via Github, always open for internal PRs) to the types of questions we ask (Do we ask behavioral, experience-based, or skill-based questions? How many of each? To which candidates should we ask each set of questions?) to the way that we structure interviewer feedback. In the following sections, we’ll describe the pillars that guide our current philosophy and that we’ve used to implement the process itself.</p><h3>Interviewing is Everyone’s Responsibility</h3><p>Building a strong company is each employee’s responsibility. We expect all of our current employees to contribute to the interview process to the best of their abilities. We’re a small company, and making a bad hire can be dangerous for the organization. So we strongly believe that all members of the team should contribute. As soon as a new hire is onboarded and feels like they are comfortable assessing for the role, we start training them on our interviewing philosophy and practices. This training includes our high-level values and strategy of the company and the process itself, mechanics, and how to be unbiased and efficient at gathering information.</p><p>Ultimately, the decision to hire is made by a single hiring manager, not a hiring committee or by consensus of the interview loop. But our hiring managers are selected for their ability to synthesize feedback into a cohesive whole and for their ability to raise the bar in the quality of the information that they’re receiving. Some hiring managers even grade and critique the quality of the feedback they’ve been given from each interviewer as well as the mechanics and quality of the interview itself.</p><h3>Potential for Impact is a Top Priority</h3><p>The ability to make a positive impact is by far the most important quality we use to evaluate our employees, and therefore also our interview candidates. Is someone smart and can they get things done? That’s great to see, but only a piece of the puzzle. Resilience, big-picture thinking, the ability to work with others, and the ability to work in a cross-functional team and make everyone better are also important, as are breadth of strong software engineering skills. We ask candidates to demonstrate that they can read, write, debug, architect, and refactor both code and systems at a level we’d expect from someone with their experience.</p><p>While hard skills have a well-defined set of expectations, some softer qualities are more difficult to evaluate in a candidate. To do this to the best of our ability, we’ve developed a set of questions using the STAR (Situation, Task, Action, Result) framework, and we devote at least some portion of the interview day to evaluating these qualities. For management candidates we weight this more heavily, but even for non-management roles we expect contributors to ask hard questions, challenge assumptions, and ensure that we’re solving the right problems.</p><h3>Candidates Should Interview Us Too</h3><p>Our product solves problems that many candidates may not be familiar with. We don’t require prior financial experience (although it is a plus!) and unless the candidate has previously been a financial advisor, they might not understand the particular pain points we solve for. We make sure to start each interview day with a product demo and do our best to ensure that the interview slate is comprised of employees from different parts of the organization: both of these practices allow candidates to ask questions to a diverse group of people who will have different viewpoints and daily work lives. We also ask every interviewer to make sure that they plan to eat lunch at the same time as the candidate to allow the candidate to ask questions in a more informal setting.</p><h3>Be Fair</h3><p>We hate gotcha questions as much as anyone else does and we do our best to never ask them. However, the realities of many of our roles require that we assess skill and ability to contribute in hard technical areas and to complex problem solving. We ask our candidates (even for leadership roles) to write working code, to design systems and write maintainable code, debug and refactor, and to solve abstract problems on the whiteboard. While questions that require solving abstract problems on a whiteboard have a mixed reputation in the industry, they are the best way we know of to evaluate these skills. We do design many of our whiteboarding questions around actual problems that we’ve had to solve at Addepar and while every employee might not need to solve the particular problem we asked, we want to make sure they could if called upon.</p><p>We do our best to be as fair as possible in these situations. We allow candidates access to a laptop with internet connectivity and the candidate’s choice of editor or IDE and encourage candidates to ask for any additional resources that they may need. We train our interviewers to set expectations for solutions clearly to limit the chance of a candidate misinterpreting a problem, and to offer help and guidance on the question if the candidate asks for it.</p><h3>Experience is a Strong Positive Signal</h3><p>We love candidates that have experience and can show that they’ve learned from it. We look favorably on candidates that have built financial data systems and portfolio reporting applications, can demonstrate significant impact in previous roles, and can demonstrate an advanced engineering approach. If it’s not clear from the beginnings of this post, we love learning — and there’s no stronger “hire” signal than someone who has something to teach. Our day-to-day requires synthesizing knowledge of systems and processes that have been around for the last few decades with forward-thinking approaches that will lay the foundation for the next few and a deep understanding not just of modern frameworks and trends but also of how to best apply them to real business problems.</p><p>If this sounds exciting to you, <a href="https://addepar.com/careers/">we’re hiring</a>!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*HpyMCU9XD6KOoCLcF179fQ.jpeg" /></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=9ef77b6bd11b" width="1" height="1"><hr><p><a href="https://medium.com/build-addepar/how-we-interview-at-addepar-9ef77b6bd11b">How We Interview at Addepar</a> was originally published in <a href="https://medium.com/build-addepar">Build@Addepar</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Type Workshop]]></title>
            <link>https://medium.com/build-addepar/type-workshop-dba1c06147cb?source=rss----596e43e5e150---4</link>
            <guid isPermaLink="false">https://medium.com/p/dba1c06147cb</guid>
            <category><![CDATA[design]]></category>
            <category><![CDATA[typography]]></category>
            <dc:creator><![CDATA[Desirae Rivera]]></dc:creator>
            <pubDate>Tue, 03 Oct 2017 13:26:00 GMT</pubDate>
            <atom:updated>2017-10-09T21:50:17.818Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*u5LnDK05Tq4kwdoR8Oh6BQ.png" /></figure><h3>Typography is for everyone</h3><p>Typography speaks to instinct. Often an unnoticed element, type can serve as the biggest influence in a design because it tends to tell the personality of a piece before the copy’s been read. To share the love of type with the company, Addepar’s Design team hosted an in-house, hands-on workshop with Rod Carvazos, owner of <a href="https://www.psyops.com/">PSY/OPS Type Foundry</a> in San Francisco, California.</p><h4>About Rod + PSY/OPS Type Foundry</h4><p>PSY/OPS specializes in custom typefaces for startups. It also creates typefaces based on logos for larger corporations like Taco Bell, Amazon, Dr. Seuss books, and Jack Daniels. PSY/OPS uses <a href="http://www.fontlab.com/">FontLab</a> to create type since, compared to Adobe Illustrator, it’s easier for typographers to draw curves. PSY/OPS can typically create an entire font family in about a month–a lightning-fast pace that only years of experience and a lot of hard work can accomplish.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*jkBGTZyJmwcz0U95.jpg" /></figure><p>Rod’s dedicated his career to the intricacies of type, and he sees all type as equal and beautiful. If a typeface looks overdone or bad, he thinks it’s because it’s been misapplied. For example, Rod believes Comic Sans has such a bad reputation because it’s been used incorrectly for so many years; Comic Sans was created for comic books, not government documents. For that purpose, it’s actually a very well made typeface.</p><p>Rod’s process is simple: he starts with one letter and builds the entire font from there. A true kid at heart, he told us that he loves and cares for letters as if they’re his children, or imaginary characters in a story he gets to bring to life. Whenever he has the chance, he creates fun objects to interact with the letters. With reluctance, he explained that his least favorite letter is “W” (but don’t let W know that, of course). It’s a tricky letter to draw because it’s often wider than the every other letter. He loves “K” because it’s simplest to transition into other letters.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*4m1IBB7PhMblypf8.jpg" /></figure><h4>Our Workshop Experience</h4><p>Rod kicked off the workshop with a session just for our Design team. He introduced blocks and puzzles with unique typefaces, and we “oohed” and “awed” while spelling out silly words. He reminded us to have fun with type and not take it too seriously. More importantly, he reminded us to turn off our inner critic.</p><p>After goofing around, we began <a href="http://fontbake.me/">Fontbake: a process for creative discovery and playful collaboration founded by the </a><a href="http://psyops.com/">PSY/OPS Type Foundry</a><a href="http://fontbake.me/"> and </a><a href="http://alphabeticorder.com/">(The) Alphabetic Order</a>. It was the perfect opportunity to just draw. As we sketched, Rod reminded us of the history of typography, how written communication came to be, and how knowledge of both allows us to make educated decisions about type while remaining open to new possibilities. After completing the Design team’s Fontbake experience, we opened it up to the company. It was really great to see how different everyone’s sketches were. Drawing expertise didn’t matter; this process was just about releasing your inner child.</p><p>When we were finished drawing letters, we placed them on a table and admired our work. A week or so later, Rod sent us the working typeface for us all to remember the workshop.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*ETaF4epSwGcg-0an.jpg" /></figure><h4>Using Type to Improve Our Product</h4><p>After the workshop, we picked Rod’s brain about Addepar’s use of Univers, an expansive font family that lends itself well to a variety of uses. A font family is the entire system of styles for a typeface that encompasses bold, thin, italic, etc.</p><p>Our issues with Universe are that a) the rendering of the dot on the lowercase “i” sometimes makes it look more like a lowercase “l”, and b) the rendering of bold text causes legibility issues. Ideally, the font family should remain legible no matter the weight. Rod pointed out that the owner of the font (Monotype) will generally work with clients to fix these kind of issues. If we’re unable to fix them, we’ll look into creating our very own font.</p><p>Overall, the workshop was a success. We had a lot of fun while learning more about an integral part of our work. We hope it was just as rewarding for those who attended, and we look forward to hosting more events like this in the future.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=dba1c06147cb" width="1" height="1"><hr><p><a href="https://medium.com/build-addepar/type-workshop-dba1c06147cb">Type Workshop</a> was originally published in <a href="https://medium.com/build-addepar">Build@Addepar</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Rendering 10 Million Items with {{vertical-collection}} and Ember.js]]></title>
            <link>https://medium.com/build-addepar/rendering-10-million-items-with-vertical-collection-and-ember-js-3b8a4342cc84?source=rss----596e43e5e150---4</link>
            <guid isPermaLink="false">https://medium.com/p/3b8a4342cc84</guid>
            <category><![CDATA[frontend]]></category>
            <category><![CDATA[vertical-collection]]></category>
            <category><![CDATA[ember]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[open-source]]></category>
            <dc:creator><![CDATA[pzuraq]]></dc:creator>
            <pubDate>Tue, 03 Oct 2017 13:21:00 GMT</pubDate>
            <atom:updated>2017-10-03T13:21:00.887Z</atom:updated>
            <content:encoded><![CDATA[<p>Here at Addepar we push the limits of what the browser can do on a fairly regular basis. Our application has to render large amounts of aggregated financial data to our users quickly and efficiently, which means we have to pull out all the tricks to squeeze every last bit of performance out of the Javascript VM.</p><p>For example, our open source addon Ember Table uses occlusion, a sort of technical sleight-of-hand, to render only the rows that the user is currently viewing in the table. If we were to load, say, 10 million rows, only the 40 or 50 that were visible on the screen would actually be rendered and incur the cost of instantiating the Ember components and DOM elements. As the user scrolls, the rows are recycled and seamlessly rendered on the other side without them noticing a thing.</p><p>Addepar is currently on Ember 1.8, and as such our infrastructure and libraries are showing their age. We recently began working toward upgrading our main app to Ember 2, a long and arduous journey that will be the topic of many upcoming blog posts. One of the first tasks we decided to take on was updating Ember Table to a modern, component-oriented addon, and to do that we needed to find a suitable replacement for the occlusion layer. Originally, we used Ember.ListView, but that view was deprecated and stopped working around Ember 1.11. The replacement had to:</p><ul><li>Be compatible and tested all the way back to 1.11</li><li>Be performant with collections of at least 20,000 items (the current maximum we display). Ideally, the library would support even more items — raising that number is one of our client’s most common requests.</li><li>Be mature and stable, or have a path to becoming stable</li><li>Have as few constraints as possible on layout and styling. Ideally we would just be using standard HTML tables for the Ember Table rewrite.</li></ul><p>Unfortunately, there were no mature occlusion addons out there: the only ones available were both pre-1.0.0 (ember-collection and smoke-and-mirrors).</p><p>ember-collection is definitely the simpler of the two, but it has some stringent requirements. You must know the height and width of each element in the list <em>before</em> you render. In addition, because it renders and positions each item using position: absolute, we would have very little freedom in styling our elements. Finally, there was no guarantee that it would work back to Ember 1.11, and ensuring that compatibility would have required some significant reworking of the internals.</p><p>When I reached out to <a href="https://medium.com/u/9bc1af085216">Chris</a> Thoburn, author of Smoke and Mirrors, about our needs, he assured me that his end goal for the library would fit all of them. Smoke and Mirrors was going to be a highly performant without sacrificing flexibility. Its API was simple and understandable, and best of all he was determined to support 1.11 for those of us struggling to get off of Ember.ListView and move forward with Ember.</p><p>However, the catch was that the library needed a bit of work. Smoke and Mirrors has had several point releases and lots of users, but its current rendering strategy doesn’t scale well as the collection grows, and there were some bugs in general. Chris was looking for maintainers to join the HTML Next organization and take over his work on the library, breaking it apart from one monolithic, general occlusion solution into targeted individual components — vertical-collection for vertical scrolling, horizontal-collection for horizontal, grid-collection for both dimensions, and others for more general use.</p><p>Addepar has stepped up and wholeheartedly joined HTML Next and taken over the Smoke and Mirrors project, and after a few months of hard work I’m glad to announce that v1.0.0-beta.1 of our first component, vertical-collection is ready and available (just in time for EmberConf)!</p><p>In the sections that follow, I’ll dive deep into how the new and improved {{vertical-collection}} component works under the hood. I’ll also share some embedded examples in pure HTML and CSS to illustrate certain mechanics.</p><h4>Setting Up the Scroll Container</h4><p>The first part of setting up occlusion is figuring out exactly how we can use a normal scrollbar when we’re only rendering a few of the elements. Let’s say that our scroll container (which could be any element on the page, or the viewport itself) is 250px high, and we have 1000 elements that each have a height of 50px. Basic math says we should only ever have to render 6 items (250 / 50 + 1), because as we scroll we will be rendering some portion of the 1st item and some portion of the 6th item at the same time. However, if we just render the 6 items, the scrollbar will be way off — it’ll only show 50px of extra area, when it should be showing 49750px (click to view the html or css source and the results):</p><iframe src="https://cdn.embedly.com/widgets/media.html?url=https%3A%2F%2Fjsfiddle.net%2Fp9b6cqwe%2F&amp;src=https%3A%2F%2Fjsfiddle.net%2Fp9b6cqwe%2Fembedded%2F&amp;type=text%2Fhtml&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;schema=jsfiddle" width="600" height="400" frameborder="0" scrolling="no"><a href="https://medium.com/media/00352c423c178779c143cec4d2312d39/href">https://medium.com/media/00352c423c178779c143cec4d2312d39/href</a></iframe><p>The item container, the child of the scroll container, needs to be 50000px tall for us to get an accurate representation. Since we know the heights of all of our items in advance, we could manually set the height of the item container (click to view the html or css source and the results):</p><iframe src="https://cdn.embedly.com/widgets/media.html?url=https%3A%2F%2Fjsfiddle.net%2F5hn7go9k%2F&amp;src=https%3A%2F%2Fjsfiddle.net%2F5hn7go9k%2Fembedded%2F&amp;type=text%2Fhtml&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;schema=jsfiddle" width="600" height="400" frameborder="0" scrolling="no"><a href="https://medium.com/media/6414d6b2b5d72291cbf96fab164b23dc/href">https://medium.com/media/6414d6b2b5d72291cbf96fab164b23dc/href</a></iframe><p>This gets us an accurate scrollbar, but it means we also have to position those items somehow as we scroll. Instead, a better solution is to use padding in the item container to simulate the heights that the culled (unrendered) items <em>would</em> have if they were rendered. In the following simple example we “move” the items as we scroll by adding or removing 50px of padding from the top or bottom of the item container:</p><iframe src="https://cdn.embedly.com/widgets/media.html?url=https%3A%2F%2Fjsfiddle.net%2Fksezq4Lv%2F&amp;src=https%3A%2F%2Fjsfiddle.net%2Fksezq4Lv%2Fembedded%2F&amp;type=text%2Fhtml&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;schema=jsfiddle" width="600" height="400" frameborder="0" scrolling="no"><a href="https://medium.com/media/050ad5d089f7eebcc351cdfcd8955f93/href">https://medium.com/media/050ad5d089f7eebcc351cdfcd8955f93/href</a></iframe><p>Now that doesn’t look too far off from a full occlusion solution! In fact, if we just pop the item off either end as we scroll, replace its content, and pop it back on the other end, we basically have it:</p><iframe src="https://cdn.embedly.com/widgets/media.html?url=https%3A%2F%2Fjsfiddle.net%2Fg3dnvt5m%2F&amp;src=https%3A%2F%2Fjsfiddle.net%2Fg3dnvt5m%2Fembedded%2F&amp;type=text%2Fhtml&amp;key=a19fcc184b9711e1b4764040d3dc5c07&amp;schema=jsfiddle" width="600" height="400" frameborder="0" scrolling="no"><a href="https://medium.com/media/38dff25211a20036334e3c3672cbdf3b/href">https://medium.com/media/38dff25211a20036334e3c3672cbdf3b/href</a></iframe><p>This is fundamentally how the standard measurement strategy for vertical-collection now works. It’s similar to solutions used in other frameworks such as <a href="https://github.com/seatgeek/react-infinite">React Infinite</a>.</p><p>However, we also want to support variable item heights — heights which may not be know until render, and that may change when they are rendered. We also want to support any arbitrary styling that users could possibly put together, including margins (which are incredibly hard to measure due to <a href="https://developer.mozilla.org/en-US/docs/Web/CSS/CSS_Box_Model/Mastering_margin_collapsing">margin collapsing</a>).</p><p>That’s a much taller order — let’s break it down a bit.</p><h4>The Skip List</h4><p>Let’s just start with translating the above rendering strategy into one where item heights are arbitrary. When item heights are fixed, calculating the cumulative heights of the culled items is a constant time operation: numCulled * height, where numCulled = Math.floor(scrollTop / height)for the items above and numCulled = totalItems — numAbove — numRenderedfor the items below. But as soon as we have to account for variable heights things get a lot trickier.</p><p>The old version of Smoke and Mirrors created a proxy for each item in the collection, rendered or not. The proxies would track the geometries of their items — the heights, widths, top and bottom position, etc. — and update them as the user scrolled, meaning we always knew the size and position of culled items. However, this was a very heavy solution (O(n) operation per scroll) that was originally designed to support 2d and other forms of general occlusion. Because vertical-collection is restricted to one dimension, we can optimize further.</p><p>What we’re looking for is a lightweight data structure that, given an ordered list of heights and an arbitrary offset, will find the first item n in the list such that sum(1..n) &gt; offset. We could do this with a binary search on a list of the precomputed sums (e.g. given the list of heights [10, 10, 10] the list would be [10, 20, 30]). However, updating such a list would be an O(n)operation, and, since we need to be able to update the values quickly as we scroll and remeasure, it would be prohibitively expensive.</p><p>What we want instead is a <a href="https://en.wikipedia.org/wiki/Binary_tree#Types_of_binary_trees">balanced binary search tree</a>, where the leaf nodes of the tree are the individual heights, and each layer of nodes contains the sum of its children, up to the root node which contains the total height of all of our elements. Given some random offset, we can follow the tree down to get the closest item in O(log n) time, and given a height we want to update, we can follow the tree back up in the same O(log n) time.</p><p>This is the most expensive operation in vertical-collection because it runs every single time we scroll, which is approximately <em>a lot</em>.</p><blockquote>Sidenote: Anyone who’s had to suffer through a standard technical interview knows that 90% of algorithm problems boil down to “how do I binary-search it?” But I don’t think we really take the time to fully appreciate just how much time <em>log(n)</em> algorithms save us. I wanted to see just how far we would be able to push this BST strategy as we were scrolling, so I plugged in the numbers. You know how they say that if you could fold a standard sheet of paper in half 42 times it would reach the moon? Well, running a binary search across 10 million items takes 24 cycles. And for a <em>billion</em> items, it only takes 30 cycles.</blockquote><p>However, we still would have to deal with the costs of instantiating that BST — 1 javascript object per node, n nodes for each of our items, and log(n)nodes for the layers above them. This would definitely cause issues if we wanted to track 10 million+ items.</p><p>A Skip List is an alternative way of building a BST: it’s simply a 2d array of numbers, where each individual row represents a layer in the BST, and the children of a given “node” in a layer are found by doubling the index of the node into the next layer:</p><pre>[</pre><pre>  [150],</pre><pre>  [55, 95],</pre><pre>  [30, 25, 65, 30],</pre><pre>  [10, 20, 10, 15, 40, 25, 20, 10]</pre><pre>]</pre><p>Now that’s much lighter! We can improve on this data structure by using Typed arrays which allow us to use actual integers instead of Javascript’s number primitive and are much faster to instantiate.</p><p>So, now we have a way to find out where we are as we scroll, and we have a way to update that information quickly and efficiently, and it’s about as space efficient as we can get. Now, how do we measure heights of changing items? And what do we do if we rendered an item and its height wasn’t what we were expecting? Won’t that throw everything off?</p><h4>Measuring with RAF (requestAnimationFrame)</h4><p>Remeasuring items after they’ve rendered isn’t too hard: we could just schedule the task on the runloop in Ember. The tricky part, however, is when we remeasure an item that is <em>above</em> the current scroll area, only to discover that it’s a different height than we thought it was. If you recall, we set padding on top of the item container for each item that is <em>supposed</em> to be up there, but is currently culled, so we don’t know the actual height of those items until they are rendered. If the actual height was different than the expected height, the list could appear to jump around.</p><p>So, we need a way to be able to render everything, attach it to the DOM, and then make some measurements at the very last minute and make adjustments for anything that’s changed. And we need to do this fast enough that there isn’t any flickering or glitching.</p><p>Luckily, a recent browser API was made exactly for this. requestAnimationFrame allows us to schedule work that should occur just before the browser is going to paint (and finish all of it <em>before</em> it paints). Exactly what we need!</p><p>Unfortunately, Ember schedules all of <em>its</em> rendering work to happen asynchronously. First class support for RAF will probably make its way into the Ember runloop at some point, but until then we’ve hacked together a temporary custom scheduler that wraps the runloop and allows Ember to render within the animation frame itself.</p><h4>Tying it All Together</h4><p>So, we have all of the necessary components, now we just need to tie them into Ember. The main difficulty here is with Ember’s rendering engine. Glimmer is incredibly fast, but its algorithm for diffing each loops can’t make as many assumptions as we can about the nature of our rendered components. Conceptually, we know that all we will ever be doing is moving some number of components from the top to the bottom, or from the bottom to the top, and replacing their content with whatever the new items are.</p><p>We want to minimize diffing, rendering, and DOM manipulation — <em>especially</em>because we now know that all of that work must happen in a RAF, which means that if it takes too long the browser will hang very noticeably.</p><p>To accomplish this, vertical-collection only ever updates the <em>content</em> of the components for Glimmer and Ember to rerender. The ordering of non-updated components, and their content, remains intact. The changed components are then <em>manually</em> moved. (This works because Glimmer only needs to maintain a stable reference to an existing DOM node, similar to the way that ember-wormhole works.)</p><pre>1           scroll down           4                                2</pre><pre>2   -&gt;   replace 1 with 4,   -&gt;   2   -&gt;  manually move DOM   -&gt;   3</pre><pre>3         Glimmer renders         3                                4</pre><p>This operation takes a minimal amount of time, saving us a huge amount on performance <em>without</em> sacrificing flexibility in styling or structure. The final DOM is exactly the same as it would have been if there were no occlusion at all. This strategy will likely be revisited once the<a href="https://github.com/emberjs/rfcs/blob/custom-components/text/0000-custom-components.md"> Custom Components API</a> is finalized.</p><h4>Wrapping It Up</h4><p>In the title of this post I stated that vertical-collection can render 10 million items of variable, or even changing, heights quickly and efficiently. Actually, it may be able to handle more, but we weren’t able to test greater limits because Chrome started crashing when we tried instantiating 100 million <em>objects</em> before vertical-collection was even initialized!</p><p>Needless to say, if you’re looking for a way to make your app more performant and you’re rendering huge amounts of data, vertical-collection may be your occlusion solution. Check it out <a href="https://github.com/html-next/vertical-collection">on Github</a> or view <a href="http://html-next.github.io/vertical-collection">the demo here</a>.</p><p>And of course, if you’re interested in Ember.js and our open source work, drop us a line at <a href="mailto:build@addepar.com">build@addepar.com</a>!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3b8a4342cc84" width="1" height="1"><hr><p><a href="https://medium.com/build-addepar/rendering-10-million-items-with-vertical-collection-and-ember-js-3b8a4342cc84">Rendering 10 Million Items with {{vertical-collection}} and Ember.js</a> was originally published in <a href="https://medium.com/build-addepar">Build@Addepar</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>