<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[500px Engineering Blog - Medium]]></title>
        <description><![CDATA[Welcome to the 500px Engineering Blog! This is where we, the engineers at 500px, share and discuss the challenges and interesting problems we solve in our day-to-day lives. 500px is always hiring: https://jobs.500px.com. - Medium]]></description>
        <link>https://developers.500px.com?source=rss----5d9282daaaa1---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>500px Engineering Blog - Medium</title>
            <link>https://developers.500px.com?source=rss----5d9282daaaa1---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Sat, 02 Jun 2018 09:43:54 GMT</lastBuildDate>
        <atom:link href="https://developers.500px.com/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[Understanding Rendering in React + Redux]]></title>
            <link>https://developers.500px.com/understanding-rendering-in-react-redux-7044c6402a75?source=rss----5d9282daaaa1---4</link>
            <guid isPermaLink="false">https://medium.com/p/7044c6402a75</guid>
            <category><![CDATA[front-end-development]]></category>
            <category><![CDATA[immutablejs]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[redux]]></category>
            <category><![CDATA[react]]></category>
            <dc:creator><![CDATA[Michael Tighe]]></dc:creator>
            <pubDate>Tue, 13 Mar 2018 21:18:26 GMT</pubDate>
            <atom:updated>2018-03-13T21:18:26.883Z</atom:updated>
            <content:encoded><![CDATA[<figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*D-4UVCTHXivLFlQHwqbwKg.jpeg" /><figcaption>Re-rendering is not always efficient.</figcaption></figure><p>In the process of rethinking and modernizing our front-end stack here at 500px, I’ve spent the last month or so deep diving into React, Redux, and the dizzying array of tech surrounding them. As a relative newcomer to React and Redux (I know, I’m a bit late to the party), I found it a bit challenging to find a good resource clearly laying out the details of how, and more importantly, <em>when</em>, components are rendered. Understanding and controlling rendering is essential to writing good, bug free and performant React + Redux code. It is the core of the framework. Let’s get started.</p><h3>React rendering</h3><p>React renders a component whenever its props or state change. This is fairly easy to understand, but it’s actually a bit misleading; it is not the <strong>only</strong> cause of a render. Components are also rendered when their parent is rendered. In fact, the entire component tree below the updated component is blindly re-rendered, whether or not their own state or props changed at all.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Tti3AdU4OU7NPmdAZSB87Q.png" /><figcaption>When one component is rendered, all of its children are rendered as well.</figcaption></figure><p>It’s important to keep in mind that in React, rendering does not necessarily mean updating the DOM. There are two stages in React rendering:</p><ol><li>Call the render() function to render to the virtual DOM</li><li>Compare the real DOM with the virtual DOM, and update it accordingly</li></ol><figure><img alt="" src="https://cdn-images-1.medium.com/max/266/1*VuUQND-aGNQVOxAhgDfVbQ.png" /></figure><p>Since re-rendering an unchanged component does not result in any real DOM updates, it’s not as costly as you might think, especially for simple components. For the most part, it’s not a problem and you can leave well enough alone. But what can you do when it <strong>is</strong> a problem?</p><h4>Conditional rendering</h4><p>Sometimes a large number of unnecessary render calls can start to weigh down your application. React provides a hook to take control of rendering in the form of the shouldComponentUpdate lifecycle method.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/515743636b9dd7d196f5a92aab33b55e/href">https://medium.com/media/515743636b9dd7d196f5a92aab33b55e/href</a></iframe><p>React checks this method to determine whether or not to render a component, and by default it simply returns true. If you implement shouldComponentUpdate yourself, however, you can use it to check whether or not props which affect the rendered component have changed and only render if necessary.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*w-A4i6ji0n5Jd39bC1Cejw.png" /></figure><p>Be careful, though. In simple components, this process can actually be <strong>more</strong> time consuming than just rendering, especially when you start doing deep object comparisons. It also makes understanding and maintaining the component more difficult, and leaves you more vulnerable to introducing subtle bugs in the future. As with any performance optimization, you should only be doing it if you need to, and not a moment earlier.</p><h4>Pure components</h4><p>In many cases, you may want to do a simple check to determine if any props have changed before deciding to render. Rather than writing a shouldComponentUpdate method to handle this for every component, you can extend <a href="https://reactjs.org/docs/react-api.html#reactpurecomponent">React.PureComponent</a>, which will take care of it for you. You can also use a helper function like pure() from <a href="https://github.com/acdlite/recompose">Recompose</a>, which has the same effect as extending PureComponent with the added benefit of working with <a href="https://reactjs.org/docs/components-and-props.html#functional-and-class-components">functional components</a> as well. Keep in mind, though, that both methods only perform a <em>shallow</em> comparison of your new and old props. That is, if you change an attribute of an existing object in your props or state, a pure component will <strong>not</strong> notice. It will see the same object referenced in both new and old props, and conclude that nothing has changed. With some care and good practices, though, this shouldn’t be an issue — but you need to be careful.</p><h3>Working with Redux</h3><p>When you connect your component to a Redux store<em> </em>using the <a href="https://github.com/reactjs/react-redux/blob/master/docs/api.md#connectmapstatetoprops-mapdispatchtoprops-mergeprops-options">connect</a> function, which maps Redux <em>state</em> to component <em>props</em>, Redux will ensure that the props are updated and a render is triggered whenever the relevant Redux state has changed. It does this with a shallow compare, though, so Redux reducers must always return <strong>new</strong> state objects instead of simply <a href="https://redux.js.org/docs/Troubleshooting.html#never-mutate-reducer-arguments">mutating the existing state</a>. Essentially, Redux is taking care of converting your component into a <em>pure</em> component, without the need of manually implementing shouldComponentUpdate yourself.</p><h4>Managing state immutability</h4><p>Making sure you always return new state in your Redux reducers can be tricky at times. It can also be costly, involving expensive deep copying of state objects. As such, it can be helpful to use a library which enforces immutability, such as <a href="https://facebook.github.io/immutable-js/">Immutable.js</a>. This library provides immutable implementations of common collections, making updating Redux state generally both cleaner and less error prone. It also implements some fancy data structures behind the scenes which make updating state a lot more efficient than deep copying large objects or lists. It is not without its drawbacks, though, as you’ll now have to access properties through get() and set() accessor methods, and you’ll lose some nice syntax like <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/Destructuring_assignment">destructuring</a>.</p><p>As immutable objects start to spread themselves through your code, it also makes it very easy to create pure components, since your props can be reliably compared with a simple shallow equality check.</p><h3>Conclusion</h3><p>React rendering is smart, but not as smart as you think it is. If a component’s state changes and triggers a re-render, it will render all of its child components, all the way down the tree. Generally, this is fine; remember that premature optimization is the root of all evil (or thereabouts). Also remember that a render does not necessarily result in any change to the DOM, as the virtual DOM diffing will still take place. But should it become a problem, you can tame it by taking control of the decision to render with shouldComponentUpdate and PureComponent.</p><p>Redux is a bit smarter and helps ensure that connected components are only rendered when the state they care about changes, but you need to be careful when implementing your reducers to ensure they always return new values and don’t simply mutate objects, as Redux won’t notice the update. Immutable.js can help you out with this, and provide some other benefits, but again, be careful to not over-engineer your solution and add complexity and dependencies which you might not ever need.</p><p>Understanding how React and Redux decide to render components is key to writing and debugging your React projects. Knowing how to take more control over this process is equally important — just be sure to wield your new power responsibly!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7044c6402a75" width="1" height="1"><hr><p><a href="https://developers.500px.com/understanding-rendering-in-react-redux-7044c6402a75">Understanding Rendering in React + Redux</a> was originally published in <a href="https://developers.500px.com">500px Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Incremental DOM with Backbone Marionette (on Rails)]]></title>
            <link>https://developers.500px.com/incremental-dom-with-backbone-marionette-on-rails-6f372d38c0f9?source=rss----5d9282daaaa1---4</link>
            <guid isPermaLink="false">https://medium.com/p/6f372d38c0f9</guid>
            <category><![CDATA[incremental-dom]]></category>
            <category><![CDATA[marionettejs]]></category>
            <category><![CDATA[backbonejs]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[web-development]]></category>
            <dc:creator><![CDATA[Michael Tighe]]></dc:creator>
            <pubDate>Thu, 08 Feb 2018 01:21:27 GMT</pubDate>
            <atom:updated>2018-02-08T01:21:27.161Z</atom:updated>
            <content:encoded><![CDATA[<p>At 500px, the majority of our client-side code is written using <a href="http://backbonejs.org/">Backbone</a> and <a href="https://marionettejs.com/">Marionette</a>, with <a href="http://handlebarsjs.com/">handlebars</a> templates. Although it has served us well so far, we’re always on the lookout for ways to improve our stack, architecture and best practices.</p><p>As a rule, we <a href="https://developers.500px.com/a-basic-principle-for-writing-sane-backbone-marionette-code-a62d2d52bc0f">avoid directly manipulating the DOM</a> as much as possible, preferring to update state on our view component and re-render. This makes our code a lot easier to maintain, reason about, and debug. It does, however, have some downsides; it can result in re-rendering much more of the DOM than is necessary, lose scroll positions, and “jump” or “flicker” when re-rendering (depending on the elements being rendered and how they fit into the overall page). Furthermore, our approach favours moving more logic into our templates, which starts to push the limits (and intent) of handlebars.</p><p>One solution is to apply only the changes between renders to the DOM rather than completely replacing it, using something like DOM diffing or a virtual DOM. To this end, we had a look at making use of the <a href="https://github.com/google/incremental-dom">Incremental DOM</a> library during one of our hack days.</p><h3>Incremental DOM</h3><p>Incremental DOM is a library for building templates and updating the DOM in-place as required, instead of simply replacing large chunks of it. Templates are defined in JavaScript, using a handful of methods to open and close elements and create other DOM nodes.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d440b5d58f29ab96b500def621068532/href">https://medium.com/media/d440b5d58f29ab96b500def621068532/href</a></iframe><p>Clearly, this is not something you’d want to write directly, and it’s certainly not intended to be used that way. Rather, it’s designed to be a compilation target for a templating language. Once you have your template, you can update the DOM with a call to the patch method.</p><pre>patch(element, function() { render(data); });</pre><p>That’s all there is to it. There are two steps we need to figure out to get it up and running in our project:</p><ol><li>Update Marionette’s render method to use the Incremental DOM patch method</li><li>Compile our templates into Incremental DOM</li></ol><p>The first step is relatively straightforward, while the second is a bit more involved, especially given our particular setup. I’ll elaborate further on that shortly.</p><h3>Rendering Incremental DOM with Marionette</h3><p>It’s possible to override Marionette’s default render method to implement custom rendering. We were already doing this to plug in our handlebars templating, so adding Incremental DOM didn’t require much work. (Note, we’re using Marionette 2.4, so if you’re on version 3, things may look a bit different).</p><p>We need to override Marionette.Renderer.render with our new render method, and we also need to update the view’s attachElContent method to get Marionette to use the Incremental DOM patch method instead of simply replacing the DOM node.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/41b077557825cc97cdc025073db491fe/href">https://medium.com/media/41b077557825cc97cdc025073db491fe/href</a></iframe><p>Let’s quickly walk through what’s going on there. First, we define a function which we’ll use to override our view’s standard method of attaching rendered html to the DOM. Typically, this function takes a chunk of html generated by the render call as its parameter, but IncrementalDom.patch is expecting a function.</p><p>Next, we set up our custom render function. In order to allow us to have a mix of views using our standard handlebars templates as well as using Incremental DOM, we use a flag on the view named renderer to determine which method to use. If the view is using Incremental DOM, we override its attachElContent method and return a function which runs our Incremental DOM template with the current data. As mentioned above, this function is what ends up being passed into patch. For now, our template functions are attached to window with the prefix tmpl_. More on this later.</p><h3>Compiling templates to Incremental DOM</h3><p>All of our client-side templates are rendered in Rails views, on each request. This was less of a conscious decision and more a result of the evolution of our codebase over the years, and has both advantages and serious drawbacks, which I won’t get into here. We define our handlebars templates with a helper which wraps the template in a &lt;script&gt; tag.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/3188b92597cb1c8d5b11d9dfdbb49418/href">https://medium.com/media/3188b92597cb1c8d5b11d9dfdbb49418/href</a></iframe><p>There are a few libraries and templating languages out there for Incremental DOM, but none quite fit into our exact use case. As such, we opted to write a quick parser to take care of it for us, and a new helper to define our Incremental DOM templates. We write the templates in what is essentially “embedded JavaScript”. Let’s jump right to an example.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/bb4deb270471a7b9c3b6aa2f14d9fe3a/href">https://medium.com/media/bb4deb270471a7b9c3b6aa2f14d9fe3a/href</a></iframe><p>Anything in our template surrounded by &lt;% ... %&gt; is treated as JavaScript, and everything else is treated as html. We can also use &lt;%= ... %&gt; to insert the result of the enclosed JavaScript in a string or as a text node. Converting this to an Incremental DOM template, which is just a JavaScript function, is just a matter of converting html elements into Incremental DOM calls and leaving the rest, which proved to be quite straightforward.</p><p>To wrap everything up, our incdom helper passes its block into the parser, assigns the resulting JavaScript to an appropriately named function on window (in this case, window.tmpl_todo_item), and wraps it in a script tag. That’s it! At this point, we can write templates in our “embedded JavaScript” form and make full use of Incremental DOM in our Marionette views.</p><h3>Wrap Up</h3><p>In order to write <a href="https://developers.500px.com/a-basic-principle-for-writing-sane-backbone-marionette-code-a62d2d52bc0f">clean, maintainable code in Marionette</a>, we avoid manipulating the DOM anywhere outside of the render method, and prefer instead to modify view state and re-render. In order to get around some of the downfalls of this approach, we have swapped our handlebars templates and standard Marionette rendering with Incremental DOM, which updates the DOM with new changes rather than simply replacing chunks of it. We also replaced handlebars with our own form of “embedded JavaScript” templates, which allow us to use JavaScript to write much more powerful templates.</p><p>While we have yet to put this technique into production, it shows promise in solving some of the issues we have with our current methodology. In the meantime, we’re continuing to explore a number of possibilities to improve our tech stack and, in turn, our codebase.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6f372d38c0f9" width="1" height="1"><hr><p><a href="https://developers.500px.com/incremental-dom-with-backbone-marionette-on-rails-6f372d38c0f9">Incremental DOM with Backbone Marionette (on Rails)</a> was originally published in <a href="https://developers.500px.com">500px Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[AWS Elastic Beanstalk and Private Docker Hub Repos]]></title>
            <link>https://developers.500px.com/aws-elastic-beanstalk-and-private-docker-hub-repos-8ba823813642?source=rss----5d9282daaaa1---4</link>
            <guid isPermaLink="false">https://medium.com/p/8ba823813642</guid>
            <category><![CDATA[elastic-beanstalk]]></category>
            <category><![CDATA[docker]]></category>
            <category><![CDATA[aws]]></category>
            <dc:creator><![CDATA[Kevin Martin]]></dc:creator>
            <pubDate>Fri, 13 Oct 2017 04:04:14 GMT</pubDate>
            <atom:updated>2017-10-13T04:20:51.147Z</atom:updated>
            <content:encoded><![CDATA[<p>Elastic Beanstalk makes it simple to deploy an application on AWS<br>infrastructure, including automatic scaling. When this works it’s<br>good. When it doesn’t it can be frustrating to debug but we typically<br>have all the tools necessary to find the cause.</p><p>I’ve deployed an application on Elastic Beanstalk where the Docker<br>images were hosted on an Elastic Container Service repostory. This<br>went well but was only for testing purposes as I’d made<br>Beanstalk-specific changes to the Docker config file. Once this was<br>deployed and tests were successful I needed to consolidate changes to<br>the Docker config file and have Beanstalk pull from our Docker Hub<br>repository. This proved difficult due to reasons.</p><p>Originally, to use an ECS repo, I just specified the full repo name,<br>along the lines of<br>“123456789.dkr.ecr.us-east-1.amazonaws.com/service:latest”. This was<br>enough to get the cluster to pull the image.</p><p>Later, switching to using a private Docker Hub repo, things<br>changed. To do this AWS says you must push a Docker credentials config<br>file to S3 in an older format then reference this in your Dockerrun<br>file. Getting the credentials into the correct format is simple, just<br>strip an intermediate object (the “auths” key on the root object) and<br>leave its keys as keys of the root. See the docs at AWS for details:<br><a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.container.console.html#docker-images-private">http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker.container.console.html#docker-images-private</a></p><p>I tried configuring things as described by AWS. It didn’t<br>work. Deploying a new version meant to pull from a private Docker Hub<br>repo resulted in errors in the logs like:</p><pre>[Instance: i-1234345] Command failed on instance. Return code: 1<br>Output: (TRUNCATED)…px/test-service not found: does not exist or no<br>pull access Failed to pull Docker image 500px/test-service:latest:<br>Error response from daemon: repository 500px/test-service not found:<br>does not exist or no pull access. Check snapshot logs for<br>details. Hook /opt/elasticbeanstalk/hooks/appdeploy/pre/03build.sh<br>failed. For more detail, check /var/log/eb-activity.log using console<br>or EB CLI.</pre><p>Welp. That didn’t work.</p><p>I tried re-doing things a few times because you never know… and that<br>didn’t help.</p><p>Now to actually sorting things out. We’ve got an instance ID. We can<br>go from there to an EC2 instance we can SSH into (note that we’ll<br>connect as the ec2-user).</p><pre>ssh ec2-user@&lt;public DNS&gt;</pre><p>And have a look at the Docker config file:</p><pre>cat ~/.dockercfg</pre><p>Everything looks well, proper user is there. Yet the `docker pull` fails.</p><pre>cat ~/.docker/config.json</pre><p>Well, those are the old credentials from testing against a Docker repo<br>on AWS. Let’s just try that pull ourselves again:</p><pre>docker pull 500px/test-service</pre><p>Failing, as expected. What if we remove the old creds…</p><pre>rm ~/.docker/config.json<br>docker pull 500px/test-service</pre><p>Oh. So that worked.</p><p>So it turns out that this Elastic Beanstalk application was started<br>with the AWS sample application. It then was changed to pull a Docker<br>image from an AWS Docker repository. This apparently created a Docker<br>config file with the correct credentials in the location that newer<br>versions of Docker expect, at ~/.docker/config.json. Later, trying to<br>switch to use the image from Docker Hub, requires specifying a key at<br>S3 containing the Docker Hub credentials:</p><pre>cat Dockerrun.aws.json<br>[…]<br> “Authentication”: {<br> “Bucket”: “devops”,<br> “Key”: “test.dockercfg”<br> },<br>[…]</pre><p>Beanstalk then apparently pulls the credentials object at S3 and<br>stores it on the EC2 instance at the old Docker config location,<br>~/.dockercfg. When the Beanstalk scripts<br>(/opt/elasticbeanstalk/hooks/appdeploy/) tried to pull Docker<br>preferred the new config location and, when that failed, it did not try<br>to fall back to the old config location. This all makes sense but<br>definitely led to a bit of pain. Ideally the EC2 instance would be<br>cleaned up after each deploy, removing any credentials (or other<br>temporary data) to avoid such an issue.</p><p>So, I suppose the point here is that even with a system like Elastic<br>Beanstalk providing a great deal of automation to ease your<br>deployments there may still be issues. But, with a little<br>investigation, nothing is hidden from you and simple debugging<br>practices will still serve you well.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=8ba823813642" width="1" height="1"><hr><p><a href="https://developers.500px.com/aws-elastic-beanstalk-and-private-docker-hub-repos-8ba823813642">AWS Elastic Beanstalk and Private Docker Hub Repos</a> was originally published in <a href="https://developers.500px.com">500px Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Automated test optimization at 500px]]></title>
            <link>https://developers.500px.com/automated-test-optimization-at-500px-636a82b570a7?source=rss----5d9282daaaa1---4</link>
            <guid isPermaLink="false">https://medium.com/p/636a82b570a7</guid>
            <category><![CDATA[capybara]]></category>
            <category><![CDATA[continuous-integration]]></category>
            <category><![CDATA[rspec]]></category>
            <category><![CDATA[testing]]></category>
            <category><![CDATA[automation]]></category>
            <dc:creator><![CDATA[Michael Zou]]></dc:creator>
            <pubDate>Thu, 24 Aug 2017 15:58:53 GMT</pubDate>
            <atom:updated>2017-08-24T15:58:53.579Z</atom:updated>
            <content:encoded><![CDATA[<p>The primary automated test suite at 500px contains over 11000 automated examples written in Rspec with Capybara. These tests run on development branches, as well as each time a commit is merged into master. They take an average of 20 minutes to run across six threads on <a href="https://semaphoreci.com/">Semaphore</a>. We have dedicated <a href="https://boards.greenhouse.io/500px/jobs/679207#.WZ7jN4opDUI">test engineers</a> (and co-ops!) responsible for helping maintain this test infrastructure as part of the <a href="https://developers.500px.com/how-do-we-maintain-500px-1012a6fcfabd">500px quality process</a>. Reducing the run time of the test suite can have a significant impact on developer productivity. Let’s deep dive into two fixes we implemented to keep our test suite running smoothly.</p><h3>Capybara wait time</h3><blockquote>Override the default max wait time for Capybara.</blockquote><p>Capybara’s “have_selector” method is used to detect whether a given select matches an element on the page. It takes an optional “wait time” parameter. When “wait time” is not specified, it uses the configured global maximum wait time, which for us was 90 seconds. Often, tests want to verify that an element <em>doesn’t </em>exist on the page, which looks like:</p><blockquote>expect(page).not_to have_selector(…).</blockquote><p>In these cases, it’s important to consider wait times. If you don’t specify a wait time, the negated matcher will wait the default wait time before checking, so that any code responsible for removing the element has time to complete. But if the code being tested isn’t time-sensitive, this can cause large, unnecessary delays in the completion of the test. To avoid this problem, specify the wait time:</p><blockquote>expect(page).not_to have_selector(“#element”, wait: 3).</blockquote><p>This makes Capybara wait 3 seconds before checking the page for the existence of that element, rather than the default 90.</p><p>Instances of this issue can add up quickly. An audit for this issue found 6 instances of this pattern in our test suite, which was adding 9 minutes to the total run time.</p><h3>Load balancing</h3><blockquote>Don’t waste time waiting</blockquote><p>As mentioned before, we use Semaphore for our CI which runs our tests across 6 jobs. In order to optimize total run time, we need to put some thought into how we split up the tests. Previously, we balanced tests across threads based on the number of lines in the test file, but this wasn’t very effective, because the number of lines in a test file doesn’t necessarily reflect the test’s run time. Now we’re using a gem called <a href="https://github.com/ArturT/knapsack">Knapsack</a>. This gem uses a JSON manifest that keeps track of every test’s run time and uses that information to balance which jobs get which tests, allowing them to all finish around the same time. This change reduced our test suite’s average run time from 35+ minutes to 20 minutes.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*Fh4FO733O3ulIvo6EkZEVQ.png" /><figcaption>Before load balancing</figcaption></figure><p>However, this solution had a problem: over time, the JSON manifest that Knapsack depends on could get out of date, because we’re constantly adding and removing new tests. Ideally, the JSON manifest would be automatically updated.</p><p>To automate this, we needed to automatically update the JSON manifest based on the result of each test run. Since these tests run in parallel across 6 threads, we could have had each thread just write their results to the manifest. However, this proved to be impractical as it caused a lot of threadlock. Threads would finish around the same time and wait to write to the JSON manifest one by one. Instead, we decided to have all 6 threads upload their results after each run, and read those files and combine them into the complete manifest at the start of each run.</p><h3>In the end</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*CMRyuZe_f3qmI_m3bLtZrA.png" /></figure><p>Our automation now load balances itself, and updates the run times of each spec in preparation for the next run. The tests are less flaky, and timeout failures are less costly. Adding specs is effortless, and test run times are minimized. Faster tests = happy devs!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=636a82b570a7" width="1" height="1"><hr><p><a href="https://developers.500px.com/automated-test-optimization-at-500px-636a82b570a7">Automated test optimization at 500px</a> was originally published in <a href="https://developers.500px.com">500px Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A basic principle for writing sane Backbone Marionette code]]></title>
            <link>https://developers.500px.com/a-basic-principle-for-writing-sane-backbone-marionette-code-a62d2d52bc0f?source=rss----5d9282daaaa1---4</link>
            <guid isPermaLink="false">https://medium.com/p/a62d2d52bc0f</guid>
            <category><![CDATA[web-development]]></category>
            <category><![CDATA[javascript]]></category>
            <category><![CDATA[marionettejs]]></category>
            <category><![CDATA[backbonejs]]></category>
            <dc:creator><![CDATA[Michael Tighe]]></dc:creator>
            <pubDate>Thu, 10 Aug 2017 13:55:39 GMT</pubDate>
            <atom:updated>2017-08-10T13:58:40.325Z</atom:updated>
            <content:encoded><![CDATA[<p>At 500px, we use Backbone Marionette for the majority of our web application. Marionette presents a fairly straightforward framework which is easy to jump into, but while it’s more well defined than Backbone on its own, it’s not very opinionated on how exactly you should structure your app. Over time, we’ve developed best practices which help us write sane, maintainable Marionette code.</p><p>There is one basic principle that should result in mostly reasonable Marionette components:</p><blockquote>You should always be able to re-render.</blockquote><p>That is, calling render() should result in no change to the DOM. The DOM should always be a reflection of the state of the view instance. To put it another way, if we were to clone the view instance in its current state and then render it on another part of the page, both instances should render identical markup. We will identify a few rules and best practices that follow naturally from this principle and which will help you achieve it.</p><h3>Just render</h3><p>This rule could also be called “<em>don’t touch the DOM”</em>. Directly manipulating the DOM (adding/removing classes, changing styles, creating elements, etc.) in your JavaScript leads to a couple of undesirable results:</p><ol><li>Your view becomes more difficult to maintain and debug as it’s not clear where DOM state changes might be taking place.</li><li>It becomes very easy to end up with a mismatch between what the DOM is showing and the stored state of your view.</li></ol><p>When this happens, some of the state of your component is actually stored <em>only</em> in the DOM, and re-rendering the component will cause this information to be lost.</p><p>How can we avoid this? Do as much as you possibly can in the template, and if absolutely necessary, manipulate the DOM only in the onRender function (which is called by Marionette after rendering). You can make use of templateHelpers to make this a bit easier. Any time you need to change the state of the DOM, you should modify a view instance variable and re-render the view by calling render().</p><h4><strong>Bad:</strong></h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/f1117081011fa01a3f82baeba0aab9da/href">https://medium.com/media/f1117081011fa01a3f82baeba0aab9da/href</a></iframe><h4>Good:</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/1fafddd39ca728b898f4a0159936ade6/href">https://medium.com/media/1fafddd39ca728b898f4a0159936ade6/href</a></iframe><h3>Don’t save state in the DOM</h3><p>You should avoid storing state in the DOM at all costs. The DOM should reflect the state of the view instance, and you should never need to query the DOM for information. If some state is stored exclusively in the DOM (in the form of “selected” elements, checkbox states, user re-ordered items, etc.), then calling render() will cause us to lose that state, breaking our basic principle.</p><h3>Break down views into small logical components</h3><p>There are a lot of obvious and oft-mentioned benefits to breaking code up into smaller chunks. It’s easier to read and maintain, promotes code reuse, is easier to test, helps parallelize development… the list goes on. As it relates to our rule of always rendering instead of directly updating the DOM, it helps us keep the scope of renders under control and render only what is required.</p><h3>Use the Backbone router</h3><p>If your app has distinct pages, don’t overcomplicate things by listening for events and updating the page piece at a time — just use the router. Even if you have some common elements, like navigation, you can pull those into their own view components and include them in each page. You can think of this as applying the <em>Just render</em> rule to the entire page. It’s much easier to follow and understand your code if there is one, clean path to building each page or screen.</p><h4>Bad:</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/7957d82c93e5e078bb03bd0b13eec177/href">https://medium.com/media/7957d82c93e5e078bb03bd0b13eec177/href</a></iframe><h4>Good:</h4><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c7a83f997ee42eefec2513f1d58aae17/href">https://medium.com/media/c7a83f997ee42eefec2513f1d58aae17/href</a></iframe><h3>Caveats</h3><p>Of course, there are always exceptions to the rule. It is not always possible to avoid updating the DOM outside of render(). For example, sometimes re-rendering will result in a “jump” in the UI, or perhaps the loss of a scroll position. Rendering for each change also means you can’t use any transitions. Try to keep these situations limited to small sub-views in order to contain the damage, and it doesn’t hurt to leave a comment explaining why you’re breaking the general rule of thumb.</p><h3>Wrap up</h3><p>To recap:</p><ul><li>Change the DOM by updating instance variables and calling render()</li><li>Don’t update the DOM piecemeal throughout your view</li><li>Do as much as possible in the template, use onRender if necessary</li><li>Don’t save state in the DOM</li><li>Break down views into small sub-views</li><li>Use the router</li></ul><p>If you follow theses basic guidelines, and always ask yourself “if I re-rendered this view right now, would anything change?”, you’ll be on your way to writing some reasonably maintainable Backbone Marionette views. They seem straightforward, but even when you know them, they’re easy to ignore when deadlines are looming and crunch time is upon you. Your future self (and coworkers) will thank you if you resist that urge.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=a62d2d52bc0f" width="1" height="1"><hr><p><a href="https://developers.500px.com/a-basic-principle-for-writing-sane-backbone-marionette-code-a62d2d52bc0f">A basic principle for writing sane Backbone Marionette code</a> was originally published in <a href="https://developers.500px.com">500px Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How do we maintain 500px?]]></title>
            <link>https://developers.500px.com/how-do-we-maintain-500px-1012a6fcfabd?source=rss----5d9282daaaa1---4</link>
            <guid isPermaLink="false">https://medium.com/p/1012a6fcfabd</guid>
            <category><![CDATA[chatops]]></category>
            <category><![CDATA[software-development]]></category>
            <category><![CDATA[continuous-integration]]></category>
            <category><![CDATA[500px]]></category>
            <category><![CDATA[test-automation]]></category>
            <dc:creator><![CDATA[Shreya Khasnis]]></dc:creator>
            <pubDate>Fri, 28 Apr 2017 15:25:26 GMT</pubDate>
            <atom:updated>2017-04-28T15:35:50.580Z</atom:updated>
            <content:encoded><![CDATA[<p>500px users may notice new features on our site occasionally, but we put new code into production multiple times a day! How do we ensure that everything works after every change? Good question! I’m a member of the quality assurance (QA) team at 500px — we play a big part in this. Here are some of the development techniques we use to ensure a smooth sail.</p><p>First of all, code changes are reviewed by other developers before they even reach the QA team. This often catches bugs and corner cases early in the process. Code review also helps reduce redundant code and encourages syntactic consistency, both of which keep our codebase clean.</p><h3><strong>Test Automation</strong></h3><p>Pull requests are reviewed by developers, but also reviewed by machines! We have a large suite of automated tests, which run when new pull requests are opened. These tests are a great way to ensure that new features work as expected and verify that these new changes do not break existing functionality. Given the variety of features on our site, it would be time-consuming to test all aspects by hand on every code change. Currently, we have about 4,000 automated tests that are separated into threads which run simultaneously. We use a continuous integration framework called Semaphore CI that runs these tests on every proposed change. The tests are randomly executed, which encourages the development of independent tests to ensure the order of execution does not impact the expected result. This helps us parallelize the test suite into different threads. Semaphore can also be integrated with Slack to inform developers about tests that have passed or failed. From this, developers are able to triage through and fix the code that broke things.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/204/0*bh5YavY6MDdU6NzK." /><figcaption>Semaphore CI, a continuous integration framework that we use for automation.</figcaption></figure><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*7PKQ4vN3Q_nnz4XyGdIhMA.png" /><figcaption>One of the best safety nets is having a passing master branch!</figcaption></figure><p>For a more in-depth overview of how we integrate and use Semaphore check out <a href="https://semaphoreci.com/blog/2014/09/29/inside-development-at-500px.html">this link.</a></p><p>From my perspective as a QA developer, this automated testing allows us to focus on capturing edge cases of new product requirements and spend less time reviewing previously-tested functionality. The automated test cases are reusable: they eliminate the need to manually retest all features. We also create new tests and update older ones to keep the test specifications up-to-date and make sure they accurately reflect the behaviour of 500px.</p><p>But test automation does not cover everything! Each change is also manually tested to catch any remaining bugs. This includes positive testing (confirming that everything works when it should) and negative testing (confirming that errors are handled gracefully). Since 500px caters to many users, testing with a diversity of user accounts (e.g. both free users and paid members, or different language preferences) is crucial! For the QA team, testing with multiple user accounts helps us verify that features are accessible to the correct group of users and catch discrepancies between translations.</p><h3><strong>ChatOps</strong></h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/239/0*Ll016nCwy7gGEJVW." /><figcaption>BMO, our bot named after the famous Adventure Time character.</figcaption></figure><p>Generally, QA has the final say about when changes are ready to be deployed. Our Slack bot, BMO, can deploy changes to all our different testing and production environments. BMO encourages a process known as conversation-driven development and promotes greater transparency between team members. It’s also very easy for new team members to deploy changes! All they do is type a command such as “bmo deploy 500px” into Slack and BMO begins the deployment. BMO also notifies the user of any deployment errors as well as successful deployments and includes a log to help any debugging.</p><p>The QA team uses BMO’s ability to deploy to different environments to view code changes before they go live. Since we have access to multiple pre-production environments, BMO also tracks which ones are in use and notifies team members when someone is using an environment. This prevents people from inadvertently using the same environment and affecting their tests.To use an environment, we type “bmo lock <strong>environment” </strong>and “bmo release <strong>environment” </strong>in Slack.</p><p>To know more about the mechanics of BMO and how it was implemented, check out the <a href="https://developers.500px.com/chatops-f07c15d7749c#.jv2yrimsw">ChatOps</a> segment earlier on in the 500px developer blog.</p><p>Overall, that’s a quick summary of how we move fast and introduce new features without breaking things. If you have any questions, leave a comment below and check out our other blog posts for more details on how we work. And if this is interesting to you and have suggestions on how we can make our setup better: <a href="https://about.500px.com/jobs/">we’re hiring!</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/113/1*WCVQkEoA_4r0qNo3pK9CQw.png" /></figure><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=1012a6fcfabd" width="1" height="1"><hr><p><a href="https://developers.500px.com/how-do-we-maintain-500px-1012a6fcfabd">How do we maintain 500px?</a> was originally published in <a href="https://developers.500px.com">500px Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Consistency Managers & Communication Channels — RocketData as a Messaging Bus]]></title>
            <link>https://developers.500px.com/consistency-managers-communication-channels-rocketdata-as-a-messaging-bus-78ff2ba31622?source=rss----5d9282daaaa1---4</link>
            <guid isPermaLink="false">https://medium.com/p/78ff2ba31622</guid>
            <category><![CDATA[consistency]]></category>
            <category><![CDATA[mobile-app-development]]></category>
            <category><![CDATA[ios]]></category>
            <category><![CDATA[swift]]></category>
            <dc:creator><![CDATA[Kevin Truong]]></dc:creator>
            <pubDate>Fri, 28 Apr 2017 14:11:11 GMT</pubDate>
            <atom:updated>2017-04-28T18:46:27.292Z</atom:updated>
            <content:encoded><![CDATA[<p>I’m a Computer Engineering student at the University of Waterloo, currently working at 500px as a mobile developer for my co-op term. This term I’ve primarily been working on the 500px iOS app, and specifically tackled the problem of consistency management.</p><p>Consistency management is a common problem for mobile apps. When a user interacts with the app, they expect that their actions will be reflected consistently throughout all areas of the interface. For example, if a user deletes their photo, they expect to see this photo removed from other views of the app, such as their profile or the home feed. Achieving consistency can be problematic for the views retained in memory, where the data can become outdated. Consistency is easy to take for granted — but when it’s not there, users notice!</p><p>In 500px for Android, we <a href="https://developers.500px.com/managing-consistency-on-android-8767817d057a">use Jackie to ensure consistency</a>. We wanted a similar solution for iOS, so we started off looking into what was already out there. After some research, we found <a href="https://github.com/linkedin/RocketData">RocketData</a> from LinkedIn. RocketData provides similar functionality to Jackie, and both solutions were inspired by the same <a href="https://www.youtube.com/watch?v=XhXC4SKOGfQ&amp;feature=youtu.be&amp;t=6m47s">Facebook talk</a>. RocketData has some nice benefits over Jackie: it handles concurrency slightly better, is explicit about what thread a function will run on, and it nicely handles most threading issues automatically. RocketData also supports disk caching, whereas Jackie only supports an in-memory cache. Overall, they both satisfy the core functionality required to solve the consistency problem.</p><p>After playing around with RocketData, we realized that it could be used as a message bus to solve communication issues between various parts of the view hierarchy. Let’s dig into the details of both approaches, and describe why we ended up with the RocketData message bus approach.</p><h4><strong>Communication Channels</strong></h4><p>A key part of consistency management is communicating state changes throughout different components of our app. When there is a change in a data model, this change needs to be propagated to other parts of the app, so that other view controllers can update their UI to reflect the change. In some older parts of our app, we use NSNotificationCenter as a message bus to call different view controllers to perform some action, and we use the delegate pattern to handle communication between UIViews and their view controllers.</p><h4><strong>Delegate Pattern</strong></h4><p>The delegate pattern is a good way of handling communication between UI views and their view controllers. However, it can create long delegate chains when you have deeply nested view hierarchies. We encountered this issue when we started using IGListKit to simplify our massive collection/table view controllers. IGListKit allowed us to decouple our UI into different components: view controller, section controller and UIViews. The view controller managed all the section controllers, and the section controllers managed the different type of cell views that we presented.</p><p>Here’s a simple example of the long delegate chain problem: the user clicks a button in the app. The button is in our UIView, which will have to call a delegate function that’s implemented in the section controller. However, the operation that we want to perform is in our view controller, i.e. dismiss the cell. Therefore, we need to make another delegate call. The section controller ends up just being a messenger between our UIView and the view controller.</p><p>Although using IGListKit made our code more modular, cleaner and easier to debug, it introduced these long delegate chains which added a lot of duplicate code and were annoying to debug. It’s also wasteful because a section controller might have to store a delegate property that it may never use, but simply passes to one of its child views. First, we considered using NSNotificationCenter as an alternative to using delegates.</p><h4>NSNotificationCenter</h4><p>With the NSNotificationCenter approach, we register observers for particular events, and send NSNotification events to trigger those observers. When notified, the observer calls its selector function to handle the event.</p><p>One advantage to using NSNotificationCenter is that there is <a href="https://objcsharp.wordpress.com/2013/08/28/why-nsnotificationcenter-is-bad/">no coupling</a>, so it makes it easy to swap out code. However, the NSNotificationCenter approach has several downsides:</p><p>First, you need to make sure that you unregister all your observers or you risk having bad pointer crashes. This is <a href="https://useyourloaf.com/blog/unregistering-nsnotificationcenter-observers-in-ios-9/">no longer a problem with iOS 9 and up</a>, but you still need to unregister observers for older iOS versions, as well as block based observers.</p><p>Second, there is the risk of deadlock: if two observers broadcast notifications, and are listening to each other’s notifications, we will deadlock. The usual fix for this is to ensure that observers aren’t broadcasting any messages, but that is error-prone and often leads to messy workarounds.</p><p>Third, NSNotificationCenter makes debugging difficult. When debugging there is <a href="https://objcsharp.wordpress.com/2013/08/28/why-nsnotificationcenter-is-bad/">no line-of-sight access between the observer and the notification sender</a>. So if we were to place a breakpoint in the function that our observer calls, we can’t navigate the call stack to where the notification was sent to check the state of the app at that point.</p><p>Those are just a few of the common issues with using NSNotificationCenter. We’ve experienced these problems first-hand in our app in the past, so we wanted to find a better alternative.</p><h4>RocketData</h4><p>To use Rocket Data as a message bus, we used RocketData’s DataProvider and DataModelManager classes. From <a href="https://engineering.linkedin.com/blog/2016/07/rocket-data--faster-model-management-for-ios">LinkedIn’s post</a>:</p><blockquote>“Any models that are added to a data provider become “managed” models. Rocket data will ensure that these models are kept in sync with the cache and any other data providers”.</blockquote><p>DataModelManager and DataProvider ensure that when one part of the app updates a model, the change is automatically reflected in other parts of the app. We decided to try using this functionality as a message bus by storing event objects in the DataProvider. I’ll use one of our collection view controllers as an example.</p><p>First, we create a DataProvider in our ActivityViewController that holds a refresh event model (ActivityViewControllerEvent).</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/2f9274ff7470b6ab6739d4e1cc865b5e/href">https://medium.com/media/2f9274ff7470b6ab6739d4e1cc865b5e/href</a></iframe><p>In our viewDidLoad function, we instantiate a refresh event model and pass it to the DataProvider. We also set the DataProvider’s delegate to be the ActivityViewController:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a45475743e8cc07effb73270b55bde0f/href">https://medium.com/media/a45475743e8cc07effb73270b55bde0f/href</a></iframe><p>Next, we implement the DataProviderDelegate protocol to handle changes to the model:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/0af6a3aff717cbddcacf7ebf09eb450f/href">https://medium.com/media/0af6a3aff717cbddcacf7ebf09eb450f/href</a></iframe><p>When we use DataModelManager to make a change to the refresh event model that is stored in the consistency manager, it will call the DataProviderDelegate in our ActivityViewController to handle this change. In order for our DataModelManager to detect changes between the model in the consistency manager and the model that we’re passing into DataModelManager, we need to implement the == function.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/4e4dd54ec90d2d54299337f87e8600a5/href">https://medium.com/media/4e4dd54ec90d2d54299337f87e8600a5/href</a></iframe><p>In our case we give the ActivityViewControllerEvent a timestamp property, set to the current date when the model is created. This timestamp property is used by DataModelManager to compare the model to other models. Since the timestamp is set at creation, every model will be different. So when we use DataModelManager to update our refresh event model, it will always see that the objects are different and notify our DataProviders that there has been a change.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/c38e0692963aaa95a71ce944eb0d58e5/href">https://medium.com/media/c38e0692963aaa95a71ce944eb0d58e5/href</a></iframe><p>In our case, the DataProvider in ActivityViewController will be notified and will call its DataProviderDelegate to handle the change.</p><p>RocketData directly addresses the pain points from the NSNotificationCenter approach: there’s no risk of memory leaks, it’s easier to debug, and we don’t need to worry about unregistering our DataProviders. When we call updateModel from DataModelManager, the response from the DataProviderDelegates responds on the same thread, which simplifies things, and it automatically guarantees consistency.</p><p>We’d love to hear about how others have solved these problems or tried out similar solutions, so drop us a note and let us know what you think!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=78ff2ba31622" width="1" height="1"><hr><p><a href="https://developers.500px.com/consistency-managers-communication-channels-rocketdata-as-a-messaging-bus-78ff2ba31622">Consistency Managers &amp; Communication Channels — RocketData as a Messaging Bus</a> was originally published in <a href="https://developers.500px.com">500px Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Tips and Tools from the 500px web team]]></title>
            <link>https://developers.500px.com/tips-and-tools-from-the-500px-web-team-f63ac9ae39cd?source=rss----5d9282daaaa1---4</link>
            <guid isPermaLink="false">https://medium.com/p/f63ac9ae39cd</guid>
            <category><![CDATA[coding]]></category>
            <category><![CDATA[developer]]></category>
            <category><![CDATA[developer-tools]]></category>
            <category><![CDATA[web-development]]></category>
            <category><![CDATA[software-development]]></category>
            <dc:creator><![CDATA[Javier Ruiz]]></dc:creator>
            <pubDate>Fri, 16 Dec 2016 18:57:55 GMT</pubDate>
            <atom:updated>2016-12-21T15:54:06.971Z</atom:updated>
            <content:encoded><![CDATA[<p>The 500px web team meets weekly for a broad-ranging discussion we call the “Web Tech Deep Dive”. Sometimes we explore a dark corner of our codebase; sometimes someone presents a new technology; and sometimes it’s a round-table discussion. We recently shared tools and tricks we use to be more productive, and thought it would be a great idea to share some of these with you! Let’s begin.</p><p><strong>Ag — The Silver Searcher</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*Xm5GTktoZf4GqUqo." /><figcaption><em>“The really clever thing about Ag is that it is smart enough to only check files that it knows about so you can tell it to ignore logs, vendor, etc. It’s super fast. There’s a whole bunch of useful little things in here”</em></figcaption></figure><p>Ag is a code searching tool for your command line, just like Ack or Grep, but better. It has the speed of Grep, and the ability to ignore files like Ack. It automatically excludes files that you’ve added to .gitignore, and groups matches by file.</p><p><em>Price: </em>Free. Who doesn’t like free stuff?</p><p><em>Link</em>: <a href="https://github.com/ggreer/the_silver_searcher">https://github.com/ggreer/the_silver_searcher</a></p><p><strong>Dash</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*B1_mRAkhcbSV29Ed." /><figcaption><em>“Super useful tool that allows me to look up docs whenever I don’t have internet available. Use it a lot when I’m commuting or on airplanes”</em></figcaption></figure><p>Dash (iOS, macOS) lets you have your own personal documentation library. It’s most commonly used to download documentation for languages and APIs to have quick access to them under a singular application. However, it also has online support allowing you to search Google and Stack Overflow for answers if you’re unable to find what they’re looking for in the docs. Some other features are snippets, annotations inside docs, and integration into most editors allowing you to quickly look up anything under your cursor. Dash is used by most members of our team.</p><p><strong>Note</strong>: Velocity is a good alternative for Windows users.</p><p><em>Price</em>: Free with the occasional prompt to purchase for $24.99.</p><p><em>Link</em>: <a href="https://kapeli.com/dash">https://kapeli.com/dash</a></p><p><strong>FZF — Fuzzy-Finder Command Line Tool</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*IrD1AvR0VMNmVk0J." /><figcaption><em>“Makes it really easy to do things like git add (specific file)”</em></figcaption></figure><p>FZF is a fuzzy finder for your terminal that’s written in Go. It lets you search for files inside the current directory or past commands in your history via blazing fast autocomplete. Bob’s most common uses for FZF are picking files to commit (`git add &lt;filename&gt;`) and edit ( `vim &lt;filename&gt;`). Ctrl+R will initialize FZF to look through your history and Ctrl+T will initialize FZF to look through all of the folders and files under the current directory.</p><p><em>Price: </em>Free.</p><p><em>Link: </em><a href="https://github.com/junegunn/fzf">https://github.com/junegunn/fzf</a></p><p><strong>Synced-Sidebar Plugin for Atom/Sublime Text</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*p04G00ZzMRxdpXhk." /><figcaption><em>“It lets you easily view and switch between files in the same directory; super useful if your project structure groups files by feature”</em></figcaption></figure><p>Synced-sidebar was originally a plugin for Sublime Text but now works on Atom as well. These can be installed through the built-in package managers. It automatically syncs your sidebar to always show the currently opened file, helping you keep track of your position in your project &amp; the context for your edits. It’s a small plugin, but it affects your workflow more than you’d expect!</p><p><em>Price</em>: Free</p><p><em>Link</em>: <a href="https://atom.io/packages/synced-sidebar">https://atom.io/packages/synced-sidebar</a>, <a href="https://github.com/TheSpyder/SyncedSideBar">https://github.com/TheSpyder/SyncedSideBar</a></p><p><strong>Rubocop</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*1RRFJEYx4943Or6Q." /><figcaption><em>“Very helpful for me when writing code. If I’m looking at code that I didn’t write, I’m able to refactor code really easily because of the linter and make it look really nice”</em></figcaption></figure><p>Rubocop helps you follow your Ruby style guide throughout your code base. One of the best features of Rubocop is that it allows you to customize the types of issues you want it to catch. Once configured, Rubocop detects syntax and styling issues in your current file and highlights the issues in yellow (for warnings) or red (for errors). This lets you correct style issues as you go, rather than having to resolve them all in a pull request.. Here’s an example of our Rubocop settings:</p><p><a href="https://gist.github.com/JavierR14/c5ac03df9ffd92206244ac3c697be376">https://gist.github.com/JavierR14/c5ac03df9ffd92206244ac3c697be376</a></p><p><em>Price: </em>Free</p><p><em>Link: </em><a href="https://github.com/bbatsov/rubocop">https://github.com/bbatsov/rubocop</a> , or download through your editor’s package manager by looking for ‘linter-rubocop’</p><p><strong>Spectacle</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*eCfI4kvrPPd-ZbzT." /><figcaption><em>“It lets me adjust my screens really quickly since macOS doesn’t have built-in support for snapping screens to the sides or resizing them easily. I can do a lot of different adjustments on the fly”</em></figcaption></figure><p>Spectacle is a general purpose tool that isn’t specific to developers, but it’s incredibly useful for Mac users. macOS doesn’t have any built-in shortcuts to resize windows or snap them to the borders of your screen. Spectacle makes it easy to define shortcuts that move, resize, and snap windows in position. Personally, I use the default shortcuts for snapping windows to different sides of the screen as I find them convenient. However, every shortcut is customizable.</p><p><em>Price: </em>Free. They have a donation link if you’d like to support them.</p><p><em>Link: </em><a href="https://www.spectacleapp.com/">https://www.spectacleapp.com/</a></p><p><strong>Gas Mask</strong></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/977/0*RD5CHndL-0Lz9_ys." /><figcaption><em>“I never have to bother with tracking down my hosts file and commenting out lines or adding lines just to test or access staging. I can just set multiple environments and switch between them instantly.”</em></figcaption></figure><p>Several developers and QA team members use this macOS hosts manager to easily switch between /etc/hosts configurations without manual edits. Gas Mask lets you create as many hosts files as you want and easily switch between them by double clicking (or using the menu bar icon). It’s extremely useful when you have multiple environments such as development, staging, and production where each one needs its own set of IP mappings.</p><p><em>Price: </em>Free</p><p><em>Link:</em> <a href="https://github.com/2ndalpha/gasmask">https://github.com/2ndalpha/gasmask</a></p><p><strong>Conclusion</strong></p><p>All of the tips and tools mentioned above help make your coding life more pleasant and efficient, so we recommend adding a few to your arsenal! At 500px, we’re always looking to grow and expand our knowledge. What do you do personally to be more productive as a developer? Leave a comment below and let us know about your favourite tools or tips!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f63ac9ae39cd" width="1" height="1"><hr><p><a href="https://developers.500px.com/tips-and-tools-from-the-500px-web-team-f63ac9ae39cd">Tips and Tools from the 500px web team</a> was originally published in <a href="https://developers.500px.com">500px Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[UDP Load Balancing with Keepalived]]></title>
            <link>https://developers.500px.com/udp-load-balancing-with-keepalived-167382d7ad08?source=rss----5d9282daaaa1---4</link>
            <guid isPermaLink="false">https://medium.com/p/167382d7ad08</guid>
            <category><![CDATA[500px]]></category>
            <category><![CDATA[keepalived]]></category>
            <category><![CDATA[devops]]></category>
            <category><![CDATA[networking]]></category>
            <category><![CDATA[load-balancing]]></category>
            <dc:creator><![CDATA[Matt Stobo]]></dc:creator>
            <pubDate>Fri, 16 Dec 2016 19:07:44 GMT</pubDate>
            <atom:updated>2016-12-21T15:04:10.302Z</atom:updated>
            <content:encoded><![CDATA[<p>About halfway into my four month internship as a platform developer at 500px, I was faced with the problem of load balancing UDP packets. The rate limiting service that I had been writing was ready to ship: the code was finished, the Chef cookbooks were written, and the servers were provisioned. The only thing left on my plate was to come up with a load balancing solution. I needed to make sure that the rate limiter didn’t get overwhelmed with packets. There was no precedent in our codebase; this was our first service serving a large amount of UDP traffic. I had a clean slate to choose whatever implementation I wanted.</p><p>A couple of options quickly surfaced. The first was Keepalived, a load balancer built on top of the <a href="https://tools.ietf.org/html/rfc5798">virtual router redundancy protocol</a> (VRRP) and the <a href="http://www.linuxvirtualserver.org/">Linux Virtual Server</a> (LVS). The other, more modern choice was everyone’s favourite proxy server, <a href="https://www.nginx.com/blog/announcing-udp-load-balancing/">nginx</a>.</p><p>Initially, nginx was the more attractive choice simply because of better (i.e. existent) documentation. In fact, if you don’t need robust health checking, Nginx is the simpler solution. However, if you need something deeper than “<a href="https://www.nginx.com/resources/admin-guide/tcp-load-balancing/">can I ping this server</a>”, you’ll have to look elsewhere. Since our rate limiter implemented an HTTP status endpoint to do more sophisticated health checks, we chose to move forward with Keepalived.</p><p>A quick read though our Chef cookbooks showed that we already had keepalived.conf file ready to go. Two HAProxy load balancers were using Keepalived as a failover mechanism (as described <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-highly-available-haproxy-servers-with-keepalived-and-floating-ips-on-ubuntu-14-04">here</a>). Basically, a VRRP-controlled IP address floated between the two machines, starting at the master and moving to the backup in case of a failure. Five different microservices were being managed under this virtual IP; it made sense to use this IP address to refer to the rate limiter as well. Only a few extra lines of configuration were needed. The outcome looked a little something like this:</p><pre># Old configuration defining the VRRP virtual IP address<br>vrrp_instance VI_1 {<br>  state MASTER         # or state BACKUP<br>  interface eth0<br>  virtual_router_id 50<br>  priority 100         # highest priority will become the master<br>  advert_int 1<br>  virtual_ipaddress {<br>    10.1.1.20<br>  }<br>}</pre><pre># Added configuration, defining two real servers under the same <br># virtual IP from above<br>virtual_server 10.1.1.20 {<br>  delay_loop 2<br>  lb_algo rr   # round robin<br>  lb_kind dr   # direct routing<br>  protocol UDP</pre><pre>  real_server 10.1.1.21 {<br>    HTTP_GET {<br>      url {<br>        path “http://10.1.1.21/status&quot;<br>        status_code 200<br>      }<br>      connect_timeout 10<br>    }<br>  }</pre><pre>  real_server 10.1.1.22 {<br>    HTTP_GET {<br>      url {<br>        path “http://10.1.1.22/status&quot;<br>        status_code 200<br>      }<br>      connect_timeout 10<br>    }<br>  }<br>}</pre><p>A reference for the Keepalived configuration can be found <a href="https://github.com/acassen/keepalived/blob/master/doc/keepalived.conf.SYNOPSIS">here</a>.</p><p>These settings define one virtual IP 10.1.1.20 as well as a set of two real servers load balanced under that virtual IP. Notice how easy it is to configure an HTTP health check! I tested out the changes on a couple virtual machines and was pleased to find that everything worked perfectly. Packets alternated between VMs just like I had told them to. It was time to ship.</p><p>Unfortunately, nothing is ever that easy. Almost immediately after running Chef Client on the production load balancers, the site went down. All of the services behind those two load balancers were unreachable using the virtual IP. Needless to say we rolled back quickly, but we were confused. The old configuration hadn’t been changed. Why would introducing new, independent functionality break our load balancers?</p><p>As it turns out, it didn’t. We had some deeper problems. After a day of digging through logs and documentation with my team, we came upon the problem. Upon examining a few of our servers, we found that their ARP entries for the virtual IP address pointed to the <em>backup</em> load balancer as opposed to the master. Packets arrived at the wrong doorstep and the secondary load balancer didn’t know what to do with them. It simply looked at their addresses, said “this isn’t for me”, and sent them away into the abyss.</p><p>Armed with this knowledge, we dug a little deeper into the inner workings of Keepalived. Whenever a server transitions to the master state, it sends a <a href="https://tools.ietf.org/html/rfc5944#section-4.6">gratuitous ARP</a> message declaring that it is now the owner of the virtual IP address. When we made our configuration changes, we caused a failover situation, causing the backup to temporarily take control. When the master came back up, it should have sent its gratuitous ARP message to reclaim control of the virtual IP. But, it turns out we had pinned Keepalived to a version with this <a href="https://bugzilla.redhat.com/show_bug.cgi?id=1077201">very</a> <a href="https://github.com/acassen/keepalived/issues/160">specific</a> <a href="https://github.com/acassen/keepalived/commit/44a885325d1b825416bab7d2fe07cf8cd6c95ece">bug</a>. Ashamed at our ops faux-pas, we upgraded to a new version. We could have shipped at this point, but we were troubled. Other dark networking problems might have been waiting, ready to take our site down once again.</p><p>It turns out that caution was to our benefit. After some more research, we came across something called “<a href="http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.arp_problem.html">the ARP problem</a>”. The ARP problem occurs when using LVS with direct routing or IP tunnelling. Since all of the machines in the LVS (ie. the load balancers and real servers) believe that they have ownership of the virtual IP address, it’s possible for clients to receive the MAC address of one of the real servers instead of the load balancer when making an ARP request. Put simply, there was a significant chance that the clients of the rate limiter were going to bypass the load balancer and access one of the rate limiters directly, rendering all of our work useless.</p><p>Luckily, there is a simple solution to the ARP problem. Linux allows dummy networking interfaces to be added to machines. A dummy interface mocks a real IP address and does not respond to ARP requests. All we had to do was add dummy interfaces with the virtual IP address to the rate limiter servers and the ARP problem was avoided. Here’s how we did it:</p><pre>$ modprobe dummy numdummies=1<br>$ ifconfig dummy0 10.1.1.20 netmask 255.255.255.0</pre><p>Our setup was starting to look pretty solid. There was one more thing we wanted to fix before we launched. In the event of a failover, we wanted to ensure connections to the load balancer wouldn’t get dropped. Rate limiting is an important piece in our infrastructure and accuracy is a key trait. Keepalived allows you to enable the <a href="http://www.austintek.com/LVS/LVS-HOWTO/HOWTO/LVS-HOWTO.server_state_sync_demon.html">LVS sync daemon</a> through a <a href="https://github.com/acassen/keepalived/blob/1e93da0f63dbeb7a3a42d72c6724c765eb20593f/doc/keepalived.conf.SYNOPSIS#L76">configuration option</a>. The sync daemon watches the master’s connection state, and continuously sends it to the backup in case a failover occurs. Our final configuration looked like this:</p><pre># Brand new config enabling the LVS sync daemon on VI_1 through the <br># eth0 interface<br>global_defs {<br>  lvs_sync_daemon_interface eth0 VI_1<br>}</pre><pre>vrrp_instance VI_1 {<br>  state MASTER<br>  interface eth0<br>  virtual_router_id 50<br>  priority 100<br>  advert_int 1<br>  virtual_ipaddress {<br>    10.1.1.20<br>  }<br>}</pre><pre>virtual_server 10.1.1.20 {<br>  delay_loop 2<br>  lb_algo rr<br>  lb_kind dr<br>  protocol UDP</pre><pre>  real_server 10.1.1.21 {<br>    HTTP_GET {<br>      url {<br>        path “http://10.1.1.21/status&quot;<br>        status_code 200<br>      }<br>      connect_timeout 10<br>    }<br>  }</pre><pre>  real_server 10.1.1.22 {<br>    HTTP_GET {<br>      url {<br>        path “http://10.1.1.22/status&quot;<br>        status_code 200<br>      }<br>      connect_timeout 10<br>    }<br>  }<br>}</pre><p>This is the final solution that is running in production today. It took some research and some understanding of lower level networking, but our setup has been very solid. We are now using this configuration for two different microservices with no issues. While there are many pitfalls that need to be avoided, Keepalived turned out to be a solid choice to load balance UDP traffic.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=167382d7ad08" width="1" height="1"><hr><p><a href="https://developers.500px.com/udp-load-balancing-with-keepalived-167382d7ad08">UDP Load Balancing with Keepalived</a> was originally published in <a href="https://developers.500px.com">500px Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Querying AWS Athena From Python]]></title>
            <link>https://developers.500px.com/querying-aws-athena-from-python-4bc0fbd611ca?source=rss----5d9282daaaa1---4</link>
            <guid isPermaLink="false">https://medium.com/p/4bc0fbd611ca</guid>
            <category><![CDATA[analytics]]></category>
            <category><![CDATA[big-data]]></category>
            <category><![CDATA[fastly]]></category>
            <category><![CDATA[presto]]></category>
            <category><![CDATA[athena]]></category>
            <dc:creator><![CDATA[Kevin Martin]]></dc:creator>
            <pubDate>Tue, 20 Dec 2016 20:46:33 GMT</pubDate>
            <atom:updated>2016-12-21T18:44:05.457Z</atom:updated>
            <content:encoded><![CDATA[<p>Amazon recently released <a href="https://aws.amazon.com/blogs/aws/amazon-athena-interactive-sql-queries-for-data-in-amazon-s3/">AWS Athena</a> to allow querying large amounts of data stored at S3. This is built on top of <a href="https://prestodb.io/">Presto DB</a>. Amazon releasing this service has greatly simplified a use of Presto I’ve been wanting to try for months: providing simple access to our CDN logs from <a href="https://www.fastly.com/">Fastly</a> to all metrics consumers at 500px.</p><p>Previously we investigated using Presto on an Elastic MapReduce (EMR) cluster to execute queries during our daily ETLs. We’d then load those queries’ outputs to Redshift for further analysis throughout the day. This was meant to avoid the cost of having an EMR cluster running all the time, or the latency of bringing up a cluster just for a single query. Athena allows us to avoid this additional cluster management as AWS is providing the always-on Presto cluster.</p><p>Some minor difficulties were encountered.</p><p>Query execution time at Athena can vary wildly. In my evening (UTC 0500) I found query times scanning around 15 GB of data of anywhere from 60 seconds to 2500 seconds (~40 minutes). During my morning tests I’ve seen the same queries timing out after only having scanned around 500 MB in 1800 seconds (~30 minutes). We’ll have to see if these become more stable over time.</p><p>Experimenting with table design brought up a couple issues as well. One of my tables with a RegexSerde took a little coaxing to nicely handle some minor log format changes we’ve done. This took awhile as the row format of a Presto table can’t be updated. Dropping the old table and creating a new one fails as it appears the <a href="https://forums.aws.amazon.com/thread.jspa?threadID=244556&amp;tstart=25">table’s schema is cached</a> for some time. This left me with creating the table with the new schema with a new name, generally just appending a number to it. Once I resolved issues with the schema I intended to simply rename the table to its final name. Again I ran into a minor difficulty: <a href="https://forums.aws.amazon.com/thread.jspa?threadID=245343&amp;tstart=0">table renaming isn’t currently supported</a> despite being documented. Oh well, I let the cached schema expire then created the table once again with its final name.</p><p>Schema resolved, I moved on to integrating with our metrics pipeline that we’ve built in Python using <a href="https://github.com/spotify/luigi">Luigi</a>. This… posed some minor challenges of its own.</p><p>Athena currently only has two interfaces: the AWS web console and a JDBC driver. Making use of the JDBC driver from Python is possible with Baztian’s <a href="https://github.com/baztian/jaydebeapi">JayDeBeApi</a> module, with a few minor tweaks. These were required as Athena’s JDBC driver doesn’t support passing some options as part of the URL but instead require they be passed as properties which were previously unsupported in JayDeBeApi. Along with this came a simple implementation of non-prepared statements — Athena does not support prepared statements yet. These tweaks are available at <a href="https://github.com/Melraidin/jaydebeapi">https://github.com/Melraidin/jaydebeapi</a></p><p>One further issue encountered while initially testing the JDBC connection was the seeming inability to query any tables outside the default DB. For now I’ve simply put our tables in the default DB and asked on the <a href="https://forums.aws.amazon.com/thread.jspa?threadID=245432&amp;tstart=0">AWS dev forums</a>; I’m sure this will be resolved soon (or I’m doing it wrong).</p><p><em>I received a response to the above issue in the linked thread. It seems that through the JDBC driver (possibly an artifact of how I’m using it from Python though) the DB name must be quoted, e.g.: </em><em>&quot;db&quot;.&quot;table&quot;</em></p><p>Querying then failed with an unexpected Java error seemingly related to type conversions. Trawling the dev forums again found a <a href="https://forums.aws.amazon.com/thread.jspa?threadID=245004&amp;tstart=0">thread mentioning an issue with the decimal data type</a>. Changing our decimal columns to doubles resolved the issue but isn’t ideal. I’m sure this will likely be fixed soon.</p><p>This brought us the ability to query Athena from Python. The last piece remaining were the base classes for Athena interactions with Luigi. These simply factor out boilerplate that would otherwise be required for Athena tasks. We have one to query Athena and store the results at S3:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/118c065e852eb0ba0f21ae96caadf125/href">https://medium.com/media/118c065e852eb0ba0f21ae96caadf125/href</a></iframe><p>One note: after querying Athena we remove any key at the result path at S3 ending with “.csv.metadata”. This is an object that is created by Athena that might be required for the Athena web console to properly display the results. It also causes problems when loading to Redshift as it will match our S3 path in the Redshift “copy” command causing a failure to load. See this <a href="https://forums.aws.amazon.com/thread.jspa?threadID=244659&amp;tstart=0">discussion</a> at the AWS forums.</p><p>After getting our results to S3 we only have to load them to Redshift. This base class makes use of an internal class to load data at S3 to Redshift but should give you an idea of the process:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/a90d08640613cc3444c21280535b085f/href">https://medium.com/media/a90d08640613cc3444c21280535b085f/href</a></iframe><p>What this all leads to is a task pipeline to load data such as logs from Fastly to Redshift that looks like this:</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/d343efe8aa1f3c8063a2db28445c6d64/href">https://medium.com/media/d343efe8aa1f3c8063a2db28445c6d64/href</a></iframe><p>This pulls data from our partitioned Fastly table through Athena. We’re able to partition the Fastly table in a form suitable for use from Athena using some undocumented options for the Fastly log location. It supports at least some of the format characters from <a href="http://man7.org/linux/man-pages/man3/strftime.3.html">strftime()</a> giving us hourly partitions to optimize our queries. Thanks to <a href="https://github.com/apeckham">Aaron Peckham</a> for this tip.</p><p>And so now we’re able to pull data from Fastly logs to Redshift for further analysis. This will let us take advantage of some of the more interesting data Fastly makes available in their logs, like geographic location.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4bc0fbd611ca" width="1" height="1"><hr><p><a href="https://developers.500px.com/querying-aws-athena-from-python-4bc0fbd611ca">Querying AWS Athena From Python</a> was originally published in <a href="https://developers.500px.com">500px Engineering Blog</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>